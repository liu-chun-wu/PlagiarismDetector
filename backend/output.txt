"\n神經網路需要非線性激活函數。如果沒有激活函數，多個神經層的運算等同於單個線性轉換：`Output = dot(w, input) + b`，其中`w`是權重，`b`是偏差。這限制了模型的假設空間，使其只能表示輸入的線性組合，即使增加網路層數也無法提升模型的表達能力。\n\n為了擴展假設空間，並使深度網路更有意義，必須引入非線性激活函數。ReLU (Rectified Linear Unit)， `f(x) = max(0, x)`，是目前深度學習中最常用的激活函數，但它存在一個缺點：當輸入為0時，對應的權重可能無法更新。\n\n其他常見的激活函數包括Sigmoid，`f(x) = 1 / (1 + exp(-x))`，其導數平滑；以及tanh (hyperbolic tangent)，`f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))`，其梯度變化比Sigmoid大，收斂速度更快，且在輸入接近0時，輸出也接近0。\n\nSELU (Scaled Exponential Linear Unit)，`f(x) = λ{x, x < 0; αexp(x) - α, x ≥ 0}`，其中`λ=1.0507...`，`α=1.6732...`，是另一個較新的激活函數。SELU 避免了輸入為0時輸出也為0的問題，並具有自歸一化特性，可以使輸入自動標準化（均值為0，方差為1），從而穩定訓練過程，並減緩梯度消失和爆炸問題，相比ReLU收斂更快。\n\n"