遞迴網路(RNN)類別神經網路在語言建模有著廣大的運用(Sutskever et al. 2011； Graves, 2013; Hermans 及Schrauwen, 2013) [27] [14] [10]，以及機器翻譯也有廣泛運用 (Bahdanau、Cho 及Bengio, 2014)[19]。RNN 能有廣泛運用最直覺的原因就是記憶區塊 可以代表序列從一開始至今的訊息。簡單RNN 的缺點是難以訓練(Bengio et al., 1994; Pascanu et al., 2013) [22] [15]，後來對RNN 模型結構提出了改進，如LSTM (Hochreiter 及Schmidhuber, 1997) 和GRU (Cho et al., 2014)[28] [25]。有許多研究是對RNN 的結構 或訓練方式提出優化 (ElHihi 及Bengio, 1995；Koutnik et al., 2014; Merity et al., 2017;) [7] [32] [36]。許多實證性研究衡量了不同RNN 類神經網路的有效性。Pascanu et al. (2014)衡量在複音音樂(polyphonic music modeling)任務、文字級別(word level)語言建 模、字元級別(character level)語言建模任務上不同結構RNN 的表現[15]。Jozefowicz、 Zaremba 及Sutskever(2015)衡量了超過一萬種不同RNN 的結構變化在各種任務上的表 現[11]，他們的結論是「如果有比LSTM 更好的結構，那這些結構一定很難被發 現。」Melis et al. (2018)比較LSTM 神經網路結構的變形(LSTM、Recurrent Highway Network、NAS)[35]，結論是LSTM 網路較優。近期一個對於RNN 的重大結構變形是 加上He et al.(2016)發現的殘差連接(Residual connection)( Pradhan 及Longpre, 2016； Prakash et al., 2016；Kim、El-Khamy 及Lee, 2017；Wu et al., 2016) [9] [37] [38] [30] [45]，改善了LSTM 的效能。
