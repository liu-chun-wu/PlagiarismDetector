In this paper, our idea is extended to discover the latent preferences of users and characteristics of items by NMF to efficiently decompose the latent matrices. Given a data matrix R = [r1, Â·Â·Â· , rn] âˆˆ â„ğ‘šÃ—ğ‘›. Suppose d dimension vectors piïƒâ„ğ‘‘ and qjïƒâ„ğ‘‘ denote the vectors of latent preference of each user and characteristic of each item, respectively. P (mï‚´d) represents the preference matrix of all user latent vectors pi, 1â‰¤ i â‰¤ m; Q (nï‚´d) represents the characteristic matrix of all item latent vectors qj, 1â‰¤ j â‰¤ n. Usually d is chosen to be smaller than m or n, so that P and Q are smaller than the time decay matrix R. NMF is a matrix factorization algorithm that find the positive factorization of given positive matrix. Assume that the given a rating matrix R consists of all records (rij). Here the goal is to factorize R into the non-negative P (mï‚´d) matrix and the non- negative Q (nï‚´d) matrix. We need to define the objective functions L that quantify the quality of approximation to minimize the functional and find an approximate factorization  ğ‘…â‰ˆğ‘ƒÃ— ğ‘„  such that : ğ¿= â€–ğ‘…âˆ’ğ‘ƒğ‘„ğ‘‡â€–ğ¹ (7) whereâ€–âˆ™â€–ğ¹ denotes the matrix Frobenius norm. Although the objective function L in Equation 7 is convex in P only or Q only, it is not convex in both variables together. Therefore, searching for an algorithm to find the global minimum of L is difficult. Lee & Seung [24,25] presented an iterative update algorithm as follows: ğ‘ğ‘–ğ‘— ğ‘¡+1 = ğ‘ğ‘–ğ‘— ğ‘¡ (ğ‘…ğ‘„)ğ‘–ğ‘— (ğ‘ƒğ‘„ğ‘‡ğ‘„)ğ‘–ğ‘— (8) ğ‘ğ‘–ğ‘— ğ‘¡+1 = ğ‘ğ‘–ğ‘— ğ‘¡(ğ‘…ğ‘‡ğ‘ƒ)ğ‘–ğ‘— (ğ‘„ğ‘ƒğ‘‡ğ‘ƒ)ğ‘–ğ‘— (9) It is proved that the above update steps will find a local minimum of the objective function L. Actually in reality, we have d â‰ª m and d â‰ª n. Thus, NMF essentially try to find a compressed approximation of the original data matrix, ğ‘…â‰ˆğ‘ƒÃ— ğ‘„ . We can view this approximation column by column as follows: ğ‘ğ‘–â‰ˆâˆ‘ğ‘ğ‘—ğ‘ğ‘–ğ‘— ğ‘‘ ğ‘—=1 (10) Where ğ‘ğ‘– is the j-th column vector of P. Thus, each data vector ğ‘Ÿğ‘– is approximated by a linear combination of the columns of P, weighted by the components of Q. Therefore, P can be considered to contain a foundation that is optimized for the linear approximation of the data in R. Since relatively few foundation vectors are used to represent many data vectors, good approximation can only be achieved if the foundation vectors discover structure that is latent in the data.
