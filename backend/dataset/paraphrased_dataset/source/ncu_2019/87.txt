「機器學習」（Machine Learning）已成為近年來熱門的詞，從政府高層到市井小 民都能輕鬆回應或解釋這個詞的意義。「機器學習」就是實現人工智慧的其中一種方式， 傳統上實現人工智慧的方式需要人們將規則嵌入到系統，機器學習則是讓電腦能夠自行 從歷史資料中學會一套技能、並能逐步完善精進該項技能 17。機器學習第一波興盛的模 型為1980 年代提出「類神經網路」、又稱人工神經網路。直到1986 年，學者包括Rumelhart、 Hinton 等人提出「反向傳播算法」(Backpropagation)訓練神經網路，先讓資料訊號通 過網路輸出結果後，計算其與真實情況的誤差（一般又稱代價函數），再將誤差訊號反 17 網站：https://www.inside.com.tw/article/9945-machine-learning 詐騙電話 詐騙地址 詐騙內容摘要 接收人聯絡電話 職業 開案編號 冒用機構 通報金融機構名稱 聯絡被害人時間 其他職業說明 受理時間 是否受騙 通報金融機構姓名 交辦單位 交付地點 案件類別 詐騙金額 通報金融機構電話 交辦回報時間 網購站名 其他案件說明 交付手法 受話號碼 筆錄單位 網購站名說明 重複案件 其他交付手法說明 受話時間 筆錄者 網購網址 詐騙管道 攔阻金額 處理情形 筆錄時間 交易序號 其他管道說明 詐騙日期 轉介縣市 三聯單號 賣家網站帳戶 詐騙手法 是否警示 轉介時間 受理單位別 買家網站帳戶 其他手法說明 詐騙帳號 接收人 學歷 建檔單位 第23 頁 向傳播回去、對每一個神經元都往正確的方向調整一下權重，如此來回個數千萬次，以 優化代價函數，找到最佳解（誤差值最小），使得具備非線性學習能力的多層感知機 (Multi-Layer Perceptron)的可能露出一絲曙光。如何找到最佳解的方法很多，最典型 的作法是採隨機梯度下降法(Stochastic Gradient Descent)。但，非線性關係的資料 因其代價函數為非凸函數，求解時容易陷入局部最佳解、而非全域最佳解，這個問題叫 做梯度消失問題(Vanishing Gradient)。[38] 隨著神經網路層數的增加而更加嚴重，意即，隨著梯度逐層不斷消散、導致神經網 路對其神經元權重調整的功用越來越小，所以只能轉而處理淺層結構（比如2 層）的網 路，從而限制了性能。然而若採用僅有兩層的神經網路，不如使用其他理論更完備也更 好實踐、同樣只有2 層的「淺層」機器學習模型。因此在1990 年代，支持向量機(Support Vector Machine)等「淺層機器學習模型」成為主流技術，此為機器學習的第二波浪潮。 以Hinton 教授為首的幾位科學家在2006 年時成功設計了第一個多層神經網路演算法， 提出限制玻爾茲曼機(Restricted Boltzmann Machines, RBM)和深度信念網路(Deep Belief Network)的概念，成功解決「反向傳播算法」優化問題。[38] 在訓綀機器學習模型時，我們會先對資料作預處理（Pre Processing），使資料一 致、完整，才能有較為正確的結果；再來是資料的萃取（Feature Extraction）與特徵 選擇（Feature Selection）；最後是模型的選取。另外，機器學習的方法有監督式學 習（Supervised Learning）、非監督式學習（Unsupervised Learning）以及增強學習 （reinforcement learning）。簡單來說，若輸入之資料有標籤，即為監督式學習，通 常用在分類與迴歸問題；資料沒標籤、讓機器自行摸索出資料規律的則為非監督式學習， 通常用在集群分析上；透過觀察環境而行動，並會隨時根據新進來的資料逐步修正、以 獲得最大利益即為增強學習，例如AlphaGo 就是增強學習的應用。[3][37]
