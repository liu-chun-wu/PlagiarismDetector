Graves 曾經探討深度LSTM 網路在手寫辨識以及文字生成議題上的優勢[2]。 Sutskever et al.曾經提出一種深度LSTM 方法，全部LSTM 層的隱藏狀態都是全連接層 [17]。Prakash et al.提出一種深度LSTM 模型[38]，上一層在第t 個時間步的的隱藏狀 態ℎ𝑡 (𝑙−1)都會傳遞給下一層的隱藏狀態ℎ𝑡 (𝑙)，此處的𝑙代表層數。在第 𝑙 層的啟動函數可 以表示為： ℎ𝑡 (𝑙) = 𝑓ℎ 𝑙(ℎ𝑡 (𝑙−1), ℎ𝑡−1 (𝑙) ) 當𝑡 = 0，𝑙 = 0，ℎ是由LSTM 等式計算出，各層的h 就由上述公式算出。 Prakash et al.從ResNet[9]中將殘差用於學習的概念，發展出堆疊殘差LSTM 網路。當 一般深度學習網路層數變深，便產生了退化現象(degradation problem)[9]，退化現象是 指當模型層數增加、複雜度增加，模型準確度反而下降的現象，與梯度消失問題不 同，殘差網路可以協助解決退化現象[30]。Kim et al.將殘差連接(Residual connection)於 輸出門[30]，Prakash et al.提出了四層堆疊殘差LSTM 網路，殘差在每兩層時用逐點加 法(pointwise addition)方式加進原網路，所以需要輸入資料與隱藏狀態ℎ𝑡有相同維度。 Sutskever et al.將前一層LSTM 網路的隱藏狀態做為下一層的輸入值[17]，而Prakash et al.將LSTM 的最後一個隱藏狀態堆疊，有著殘差連接的隱藏狀態可表示為： 圖 5 RES LSTM ℎ𝑡 (𝑙) = 𝑓ℎ 𝑙(ℎ𝑡 (𝑙−1), ℎ𝑡−1 (𝑙) )+ 𝑥𝑙−𝑛 在 l層的h是殘差 xl−n的函數，xl 代表著第 l + n 層的輸入。當n = 1，函式變成一 個有著殘差為輸入值x的簡單LSTM 網路，此時殘差網路的殘差連接並沒有增加可學習 的模型參數(learnable parameters)，也就是模型複雜度沒有增加，所以不一定要將模型 的每一層都連接。
