面臨資訊科技發展所帶來龐大的資料量，如何選擇真正相關的資訊、減少資訊量 成了機器學習領域中越來越重要的議題[15]，透過資料探勘的資料前處理階段，能夠 解決此問題，其中特徵選取為資料前處理中重要且被頻繁使用的技術之一[16]。透過 特徵選取可以去除不相關、多餘、雜訊資料[17]，精簡資料維度，使得分類正確率提 升[18]。在特徵選取方法中有四個基本的步驟，如下圖2-1 所示[19]： 1.  產生特徵子集(Subset Generation) 2.  評估特徵子集(Evaluation of Subset) 3.  終止產生特徵子集的條件(Stopping Criteria) 4.  驗證最終產生之特徵子集(Result Validation) 圖2- 1 特徵選取四個基本的步驟 (資料來源:[19]) 由圖2-1 可得知基本流程為重複1~3 的步驟直到滿足停止條件，這時所產生的特 徵子集即為特徵選取方法所決定之最佳特徵子集，最後在將此特徵子集進行有效性驗 證。 學者Guyon 和Elisseeff 將特徵選取方法分為三種類型，分別為包裝 （Wrapper）、過濾（Filters）及內嵌（Embedded）方法[13]。 1. 包裝（Wrapper） 包裝（Wrapper）方法如圖2-2 所示，歸納演算法(Induction Algorithm)可視為 一黑盒子，其主要任務是用來生成分類器，在搜尋特徵子集後，使用歸納演算法來評 估特徵子集，以選出最佳的特徵子集[20]，由於在選擇特徵子集時就使用了分類正確 率做為評估指標，通常它的表現會比過濾（Filters）方法來的好，但缺點是十分耗時 [21]，常見的包裝方法有基因演算法(Genetic Algorithm)、貝氏(naive Bayes)等。 圖2- 2 包裝（Wrapper）方法流程圖 (資料來源：[13]) 2. 過濾（Filters） 過濾（Filters）方法如圖2-3 所示，過濾方法只依照訓練資料的特性去選擇特徵 子集，選擇子集過程中並不以分類表現作為衡量指標[17]，過濾（Filters）方法的優 點是十分快速且運算簡單[22]，常見的過濾方法有主成分分析(Principal Component Analysis)、資訊獲利(Information Gain)、關聯性特徵選擇(CFS)等[11]。 圖2- 3 過濾（Filters）流程圖 (資料來源：[13]) 3. 內嵌（Embedded） 內嵌（Embedded）方法如圖2-4 所示，將特徵子集選擇的過程也成為訓練分類器 的一部分，藉此解決在包裝（Wrapper）方法中所遇到時間成本很大的問題，此方法不 只考慮輸入特徵與輸出特徵的關聯性，同時也尋找最佳化的特徵[23]，常見的內嵌方 法有決策樹(Decision Tree)。 圖2- 4 內嵌（Embedded）流程圖 (資料來源：本研究)
