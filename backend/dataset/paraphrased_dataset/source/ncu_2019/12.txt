一個單純的因果卷積只能看到與模型深度相同的歷史資訊，但是處理序列建模問 題，往往要讓模型有能力看到相當久以前的資訊。本研究採用Van Den Oord et al.(2016)建模方法，使用空洞卷積，讓模型有能力看到大小呈指數成長的接受域 (Receptive field)。對1D 的序列輸入值 𝑥 ∈ ℝ𝑛，及濾波器 𝑓∶ {0, … , 𝑘−1}  → ℝ，空 洞卷積運算子𝐹對序列中元素s 的定義如下： 𝐹(𝑠) = (𝑥∗𝑑𝑓)(𝑠) = ∑𝑓(𝑖) ∗ 𝑘−1 𝑖=0 𝑥𝑠 − 𝑑∗𝑖 其中， 𝑑 為空洞因子， 𝑘 為濾波器大小， 𝑠−𝑑∗𝑖 代表不會看到未來的資料。空洞就 代表在運算中每兩個相鄰的濾波器中加入了固定的步長。當 𝑑= 1，空洞卷積退化成一 般卷積。運用較大的空洞使得較高層的輸出可代表較寬的輸入，也代表擷取的資訊從 以前到現在含括更大的範圍，所以相當有效的增大了卷積模型的接受域(Receptive field)。 這讓我們有兩個方法可以增加TCN 的接受域(Receptive field)，一個是選擇較大的 濾波器大小 𝑘 或是增大空洞因子 𝑑 ，它們令模型其中一層的有效歷史資訊空間為 (𝑘− 1) ∗𝑑。在使用空洞卷積時較常見的作法是，隨著模型的深度增加，指數性的增加空洞 因子𝑑 (也就是對模型的第 𝑖 層而言，𝑑= 𝑂(2𝑖))。這確保了在有效歷史空間內每一個 個別的輸入值都會被某個濾波器擷取到，但同時也允許了使用深度學習的時候可以看 到非常大的有效歷史空間。TCN 模型如圖 6。 圖 6 (a) 為空洞卷積的範例，其中空洞因子𝑑= 1,2,4而濾波器大小 𝑘= 3。接受域 (Receptive field)能夠擷取輸入值內全部資料。圖 6 (b)為TCN 的Residual block， Residual block 為TCN 的組成單位。當殘差輸入和輸出的尺寸不同時，一個1 ∗1的卷積 會被加入。圖 6 (c)表示了TCN 內的捷徑連接(shortcut connection)，圖 6 (c)最右邊從第 一層連接到最後一層的連接即捷徑連接(shortcut connection)，運算方式為恆等對應 (identity mapping)。
