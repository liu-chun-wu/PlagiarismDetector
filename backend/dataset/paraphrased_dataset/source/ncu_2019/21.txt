RES LSTM 使用Prakash et al.(2016)提出的四層堆疊Res LSTM 網路，實測結果發 現RES LSTM 中，residual connection 使用恆等對應則模型容易有梯度消失的問題，所 以在residual connection 上加上了啟動函數，與本研究TCN 相同，最後加上一個輸出 層(輸出層設為線性，沒有啟動函數，1 個神經元(neuron))。RES LSTM 之殘差在每兩 層時用逐點加法(pointwise addition)方式加進原網路，所以需要輸入資料與隱藏狀態ℎ𝑡 有相同維度。全連接層及LSTM 神經層神經元數與本研究TCN 使用濾波器深度(filters) 相同，為32、128、512。各層權重初始值(kernel_initializer)設為he_uniform[8]，LSTM 神經層另外使用recurrent dropout，設為30%，整體參數如表格2。 表格2 RES LSTM 參數設定 Residual Long short-term memory Network 參數設定 神經層神經元數 kernel_ initializer recurrent dropout he_uniform 30% he_uniform 30% he_uniform 30%
