啟動函數方面，本研究與本研究將全連接層、residual connection、shortcut connection 的啟動函數設為Selu[12]，比Relu 更快收斂，解決梯度消失及梯度爆炸問 題，dropout rate 設為30%，優化器(optimizer)使用與Bai、 Kolter 及Koltun (2018)相同 設定，使用Adam[20] [31]，在使用Adam 的情況下Wu et al.(2016)將learning rate 設為 0.0002 [45]，Serrà、Pascual 將learning rate 設定為0.00001[40]，Karpathy 指出Adam 的 表現不太受到learning rate 的影響(2019)，並將learning rate 設為0.0001，本研究採用 Karpathy 設定[48]。損失函數使用均方差(MSE;Mean square error)。為了避免梯度爆 炸，與Bai et al., (2018)相同，也使用梯度裁減，使用keras 預設設定，將clipvalue 設為 0.5；clipnorm 設為1。與Lei、Zhang 及Artzi(2017)研究相同，訓練整個訓練集樣本的 次數(epochs)設為300 次[34]；參考Wu et al.(2016)研究，每次訓練使用的樣本數(batch size)設為16 [45]。為了檢視模型效率，記錄每個模型訓練時間，模型參數如表格 3。 表格 3 模型共通性參數 模型共通性參數 啟動函數 Selu dropout rate 30% optimizer Adam learning rate 0.0001 損失函數 均方差(MSE;Mean square error) clipvalue 0.5 clipnorm 0.1 epochs batch size
