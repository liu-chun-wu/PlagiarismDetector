相互資訊(Mutual Information，簡稱MI)是拿來衡量特徵變數之間相互依賴性的 程度。相互資訊越小，則代表兩個來不同事件空間的隨機特徵變數彼此之間的關聯性 越低; 相互資訊越高，關聯性則越高。如果ݐ和ܿ௜相互獨立，我們可視為ݐ不向ܿ௜提供任 何資訊，即他們特徵之間的相互資訊為零。在研究(Estevez et al., 2009; Fleuret and Ch, 2004)中提到使用相互資訊為特徵選取法對數值型資料集的計算極其敏感，即使計算中 很小的估計值誤差也會影響到特徵的選取。在研究(Battiti, 1994)中使用相互資訊演算 法於鳶尾花資料集前處理上並在分類準確度上有提升的效果，在研究(Wang and Lochovsky, 2004)中則提出相互資訊可在文本內找出依賴性較弱的文字特徵並且對於分 類器訓練的速度有明顯的提升。相互資訊計算公式如下(Battiti, 1994): MI(ݐ, ܿ௜) = ∑ܲ(ܿ௜)݈݋݃ ௉(௧,௖೔) ௉(௧)௉(௖೔) ௜ (2.9)
