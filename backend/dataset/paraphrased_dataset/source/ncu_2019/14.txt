啟動函數為神經網路增加了非線性，如果沒有啟動函數，神經網路內的神經層內的運 算將只有包含兩個線性轉換，及一個內積以及一個相加，如下所述： Output =  dot(w, input) +  b w為神經層的權重，b為殘差，如此一來神經層運算只有線性轉換，如此一來，神經層 運算後的假說空間(hypothesis space)(代表一個神經網路回傳的可能組合)就是輸入值的 全部線性轉換後的可能組合，如此的假說空間太過狹義，也不會因為層數增加增加模 型預測能力，因為很多層的線性轉換仍舊是線性轉換，增加層數不會增加模型的假說 空間。 為了讓增加的神經層數增加模型的假說空間，非線性轉換是必須的，也就是啟動函 數，Chollet, F. (2018)指出relu 是深度學習領域中，近來最熱門的啟動函數[1]。最有名 的relu(rectiﬁed linear unit)定義如下(Goodfellow et al., 2016)[2]： 𝑓(𝑥) = max(0, 𝑥) relu 函數有著如果輸入值為0，那輸入值對應的權重可能完全不會更新的缺點。 另一個常用的啟動函數sigmoid(Goodfellow et al.,2016)[2]： 𝑓(𝑥) = 1 + 𝑒−𝑥 使用sigmoid 函特點是導數較為平滑。LSTM 內的tanh 函數(hyperbolic tangent)，公式 如下： 𝑓(𝑥) = sinh𝑥 𝑐𝑜𝑠ℎ𝑥= 𝑒𝑥−𝑒−𝑥 𝑒𝑥+ 𝑒−𝑥 由於tanh 函數較sigmoid 陡峭，梯度變化較sigmoid 大，收斂速度較sigmoid 快，當輸 入值接近0 的時候，tanh 的輸出值接近0，相反地sigmoid 函數的輸出值離0 較遠。 還有個較新的啟動函數稱為selu(scaled exponential linear unit)，公式如下(Klambauer et al., 2017)[12]： 𝑓(𝑥) = 𝜆{ 𝑥       ,         𝑥< 0 𝛼𝑒𝑥−𝛼, 𝑥≥0 其中λ=1.0507009873554804934193349852946， α= 1.6732632423543772848170429916717。selu 處理了當輸入值為0，整個輸出值會變 成0 的問題，當輸入值<0，輸出值會是線性的較小的數，當輸入值≥0，輸出值會是非 線性的較大的數。selu 會自動把輸入值做標準常態正規化(平均值為0，變異數為1)， 標準化後在loss surface 上梯度的變化就比較不會忽大忽小，另外selu 也減緩了relu 函 數梯度消失、梯度爆炸的問題，也relu 更快收斂。
