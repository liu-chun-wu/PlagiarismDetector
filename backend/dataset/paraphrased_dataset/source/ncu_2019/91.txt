中文的NLP 第一項核心技術就是中文詞彙自動分割，也稱中文斷詞技術，在自然語 言處理非常有用的LDA 模型中，第一步也是必須先將文本作斷詞。目前比較常用結巴斷 詞，可自行增加停止詞（stop word）將斷詞之後無意義的詞剃除，例如在處理筆錄內 第31 頁 容時，可將問、答、報案、報案人、今日、未、至、的、於等無意義的字詞加入停止詞 （stop word）文字檔，如下圖2-8。[26][28] 圖 2-8 利用JiebaR 套件進行中文斷詞示意圖（右圖為增加停止詞後之畫面） 【圖片來源：本研究整理】 接著將中文斷詞詞彙向量化（Word Vector），或稱詞崁入（Word Embedding）， 作法是讓電腦閱讀大量文章，利用前後文的統計特性，慢慢學習出每一個詞彙的詞向量， 不必利用任何語言學知識。近年來蓬勃發展的深度學習(Deep Learning)研究，利用訓 練大量文本，使電腦可以自動調整各個詞彙的向量，解決訓練資料不足的問題，並提升 電腦的抽象化思考。訓練過程中，若我們以「詞彙向量」作統計，在向量空間上，有些 字詞間的向量很靠近，這些相近的詞彙向量在訓練資料的統計佔比大幅提升。同時，詞 彙向量在深度學習的模型之中，被視為可修改的參數，所以也具備了語義(詞彙向量)自 動調整的能力。[1]以往使用one-hot encoding 方法，將所有詞彙編號作為詞向量值， 向量的長度（維度）即為詞彙庫的大小。若詞彙庫隨著輸入資料愈來愈多，將因維度大 而浪費空間、降低處理速度，且得到的特徵是離敵稀疏的、亦無法看出詞彙與詞彙間的 關係與相似性。Word2Vec 是由Google 團隊於2013 年提出的另一種表示詞向量的方法， [41]透過訓練將詞彙庫的每個詞彙映射到一個較短的（維度較低的）詞向量，詞彙語意 愈相近的就會在空間上距離愈近，可以解決one-hot encoding 的缺點。 Word2Vec 模型又可分為CBOW（continuous bag-of-words，中文是連續詞袋模型） 和Skip-gram 兩種模型 23，以圖-12 為例，w(t)是中心詞，w(t-1)、w(t+1)則為上下詞， CBOW係利用周圍上下文作為input，去預測中心詞，Skip-gram則是以中心詞作為input， 23 參考網站: https://medium.com/@tengyuanchang/%E8%AE%93%E9%9B%BB%E8%85%A6%E8%81%BD%E6%87%82%E4%BA%BA%E 8%A9%B1-%E7%90%86%E8%A7%A3-nlp-%E9%87%8D%E8%A6%81%E6%8A%80%E8%A1%93-word2vec-%E7%9A%84-ski p-gram-%E6%A8%A1%E5%9E%8B-73d0239ad698 第32 頁 去預測上下文，也就是從一個詞彙映射為一個固定維度的連續實數向量，利用神經網路 深度學習計算詞與詞之間的距離，這個距離也就代表兩個詞之間的相似性，從而更精確 瞭解詞彙的意圖。[1] 以下列3 個句型為例，首先以CBOW 模型作說明，由上下文「以…聯繫」預測出中 心詞「LINE」、「電話」、「臉書message」語意相似，為通訊工具；若由Skip-gram 模型說明，「LINE」、「電話」、「臉書message」這3 個中心詞可以預測「以…聯繫」 是描述通訊方式，這3 個詞向量距離相近。將來若加入「我打facetime 聯繫對方」， 則語意可辨別「facetime」屬於通訊工具，詞向量與「LINE」、「電話」、「臉書message」 距離亦相近，如下圖2-9。 我和對方以LINE 聯繫後，就去ATM 匯款3 萬元。 我和對方以電話聯繫後，就去匯了3 萬元。 我和對方以臉書message 聯繫後，就去轉帳3 萬元。 圖 2-9 Word2Vec 兩種模型示意及表示圖 【圖片來源：本研究整理】[40] 第33 頁
