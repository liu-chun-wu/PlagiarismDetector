所謂的殘差塊residual block (He et al., 2016)包含了一個在轉換函數𝐹，轉換函數的 產出再加上殘差塊的輸入[9]： 𝑜= 𝐴𝑐𝑡𝑖𝑣𝑎𝑡𝑖𝑜𝑛(𝑥+ 𝐹(𝑥)) 這令神經層除了整個轉換外，還可以學習到恆等對應(identity mapping)產生的調整。由 於TCN 的接受域(Receptive field)受到模型深度𝑛、空洞因子𝑑、濾波器大小 𝑘影響，較 深、較大的TCN 模型的穩定化變得很重要。例如，預測大小212的歷史資料，同時也 是高維度的輸入序列，我們就需要一個12 層的神經網路，每一層，都包含了不同的濾 波器(filters)來提取特徵(feature extraction)。 TCN 模型也加入了殘差連接，如圖 6 (b)。在TCN 的residual block 裡，包含了兩 層的空洞卷積(Dilated convolutions)以及非線性函數，Bai et al.,(2018)在TCN 中使用 Relu 函數作為非線性函數[20]；為了要標準化，在濾波器後使用了weight normalization，同時在空洞卷積後加上spatial dropout[41]，在每一個訓練步，filter 的某 幾個channel 會歸零。 標準的Residual Neural Network 裡，輸入值會直接加在殘差函數的輸出中，在 TCN 裡，輸入值與輸出值可能有不同channel 數，為了解決這個問題，TCN 加入了1 ∗ 1的卷積，來確保element-wise 的相加都是相同channel 數，如圖 6(b)、圖 6(c)。 圖 6 TCN 模型結構的說明
