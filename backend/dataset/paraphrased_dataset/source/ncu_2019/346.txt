廣泛地說，深度學習是機器學習的一塊子領域，也就是透過多層的神經網路堆疊，進行 不斷的學習動作。其中，深度學習所擁有的自動化的特徵工程能夠更容易地解決問題(Chollet， 2017)。 一般而言，再進行機器學習時，必須要事先定義好特徵，也就是所謂的特徵工程，藉由 特徵的事先定義，給予電腦判斷的依據來去進行學習的動作。然而，在需要更深入進行學習 的網路下，特徵工程就變得十分困難。因此，深度學習透過自動化的特徵工程方式，能夠有 效提升整體效率。 然而，雖然深度學習網路非常強大，但其潛力會深深地受到訓練過程的有效性限制而改 變，例如梯度所產生的限制，在傳播過程中的誤差訊號會受到權重的大小而改變，也就意味 著設定不當可能產生梯度的消失或爆炸而影響結果(Gers,1999)。 遞歸神經網路也被廣泛的運用在時間序列上，根據Pant(2017)的研究指出，其認為RNN 的網路已經不足以應付序列的長期依賴關係，但自從LSTM 的出現後，神經網路開始能夠自 己了解什麼資訊需要被記住又什麼該被遺忘，也就導致他將這樣的網路套用在美金與盧比的 匯率預測上，為了使學習的結果更完整，從1980 到2017 的樣本也涵蓋了金融海嘯等特殊情 況，藉由一般模型與LSTM 的多次調整與比較，雖然簡單的LSTM 模型確實勝過了一般模型 的預測結果，但是卻還存在著許多的不足，例如時常低估了應有的價格跳動。由此可見LSTM 雖然看似不錯但卻仍然存在著許多問題，而根據Li(2018)等人的研究指出，透過對於輸入資 料的調整或增加的探討，也有機會能夠盡一步提高實際交易上的收益，而在相關領域中除了 LSTM 的運用，根據Bai(2018)等人的研究結果，新型的TCN 網路是另一個足以與LSTM 相 抗衡甚至準確度上能勝出的模型，因此本研究也將對其深入探討並嘗試得到好的結果。 2.3.1 啟動函數（激勵函數） 啟動函數對於神經網路是十分重要的角色，其主要目的在於讓輸入與輸出脫離線性關係。 大部分情況下，線性模型不具有足夠的表達能力，當資料無法被線性分類時，我們就需要透 過非線性函數來去轉換，將其轉換為可被線性分割的形式。 常見的啟動函數包含Sigmoid、tanh 及ReLu，其公式如下： 𝑆𝑖𝑔𝑚𝑜𝑖𝑑∶ 𝑆(𝑥) = 1 + 𝑒−𝑥 𝑡𝑎𝑛ℎ∶ 𝑡𝑎𝑛ℎ(𝑥) = 𝑒𝑥−𝑒−𝑥 𝑒𝑥+ 𝑒−𝑥 𝑅𝑒𝐿𝑢∶ 𝑅𝑒𝐿𝑢(𝑥) = max (0,𝑥) 三種常見啟動函數中，因Sigmoid 會將數據壓縮至0 至1 之間，所以廣泛被使用於分類問題， 其圖形如圖 3 所示。Tanh 則是中心為0，缺點是可能存在梯度消失問題，其圖形如圖 4 所示。 ReLu 便是為了避免梯度消失問題而出現，在深度卷積網路中，ReLu 的訓練速度更是比tanh 快上好幾倍(Krizhevsky，2012)，其圖形如圖5 所示。 圖 3 Sigmoid 函數圖形 圖 4 tanh 函數圖形 圖 5 ReLu 函數圖形 然而，對於時間序列模型來說，需要有更好更適當的啟動函數，Hochreiter (2017) 所提出 之最新激活函數SeLu，其公式如下： 𝑆𝑒𝐿𝑢∶ 𝑆𝑒𝐿𝑢(𝑥) = 𝜆{𝑥                𝑖𝑓 𝑥> 0 𝛼𝑒𝑥−𝛼  𝑖𝑓 𝑥≤0 SeLu 除了擁有更佳的收斂性，也同時能避免梯度消失與梯度爆炸，其圖型如圖 6 所示。 圖 6 SeLu 函數圖形 根據Hochreiter (2017)所提出的論點，SeLu 針對深層網路有著更佳的表現，因此本研究 決定在TCN 模型上採用SeLu，LSTM 模型則因Cudnn 的限制，只得以採用Tanh 做為啟動函 數。 2.3.2 損失函數（Loss function） 在一般的回歸問題中，通常會希望數據的殘差越小越好，也就是預測值與期望值之間的 差越小越好，因為越小代表越準確。實際上，因為誤差不會只有一項，而且有正有負，因此 通常會採用MSE（均方差）或是MAE（平均絕對誤差）做為損失函數，其公式如下： 𝑀𝑆𝐸∶ 1 𝑛∑(𝑦𝑖−𝑦̂𝑖)2 𝑛 𝑖=1 𝑀𝐴𝐸∶ 1 𝑛∑|𝑦𝑖−𝑦̂𝑖| 𝑛 𝑖=1 MSE 與MAE 在深度學習各有優缺，前者容易被異常值影響，後者則因為梯度太大容易 陷入局部最佳解，因此多數研究會採用MSE 做為損失函數。 時間序列預測所需使用到的損失函數大多為非對稱式，比起簡單的二次損失函數複雜得 多，這樣的預測會完全取決於預測誤差的分佈與損失函數本身的設計(Granger, 1999)。所以 損失函數的設計深深影響了預測的結果，我們必須事先定義一個度量來測量模型與事實的距 離(Ground truth)，才能根據這個距離來比較不同的模型(Lee, 2007)。 2.3.3 時間序列預測 由前述可知，時間序列的分析並不容易，在深度學習中也需要使用特定模型才能取得較 佳的效果，其中最著名的便是遞歸神經網路(RNN)，RNN 是一種具有內部循環的網路(Chollet, 2017)，換言之，RNN 在運作上多了將輸出層重新放回神經網路的特性（圖 7），也就造就了 類似「記憶」的特性。 圖 7 RNN 循環示意圖 資料來源：Chollet 但是RNN 的優點也成為了缺點，大部分情況下，並不是所有記憶都具有關聯，有些無 用的記憶成為了噪聲影響了學習成果，因此，改良型的長短期記憶網路(LSTM)便出現，承 襲RNN 長期記憶保留的特性，但是只會將重要的特性延續到後續作為預測使用，LSTM 有 效地提升了運作效率，但受限於硬體限制，若需要更高的運作效率，則可以使用LSTM 的 變體——GRU，藉由將LSTM 的輸入與遺忘閥合併簡化為單一更新閥來提高速度，但根據 本研究使用Cudnn 將兩者置於GPU 上運行的效率並無太大差異，因此沒有選擇使用GRU。 一般來說，卷積神經網路(CNN)雖然在圖像辨識等領域表現優良，但是在時間序列預測 上並不適當，因為其不存在因果關係的限制，無法使用。但根據Bai(2018)提出的時間卷積 網路，其特性與CNN 相似（圖 8），無需像RNN 一樣依序讀取並輸出，而是大規模並行處 理，這便能大大提升運作效率，主要的概念是透過限制卷積窗口滑動（因果卷積）來達到時 間序列預測的目的，便能同時兼具RNN 與CNN 的優勢。因此本研究最終採用此種TCN 模 型作為本次時間序列預測預測之網路。 圖 8 TCN 運算示意圖 資料來源：Oord
