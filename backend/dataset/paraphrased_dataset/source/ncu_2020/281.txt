在自然語言處理(Natural Language Processing)領域中，要對文字做分類、翻 譯等文字處理時，需將每個文字轉換為電腦可以理解的型態，即是將文字轉換 成數值形態來表示，而將文字轉換成數值形態的方法主要可分為兩類：第一類 為機器學習方法中常見的Bag-of-Words，將句子中的各個字詞表示成唯一的數 值，概念源自於Harris[21] ，屬於較傳統的將字詞轉換成向量的方法，第二類 為Distributed Representations of Words 其概念源自於Firth[22] ，該文提出若我 們想要了解一個字詞的意涵，我們必須了解該字詞的前後字詞意涵，比起Bag- of-Words 的方法更加重視的是詞與詞之間交互的關係。 Bag-of-Words 優點是運用上非常簡單且快速，缺點是代表每個詞的編碼或 權值等表示方式之間不存在任何關聯性，且當詞袋數量龐大時，單詞向量由於 是二進制的、絕大多數位數為0，會變得非常冗餘且造成記憶體的浪費，而自 從基於將詞轉換成以連續數值向量形式表示(continuous vector representation of words)方式被提出之後，詞向量亦可用來表示詞間的語意相似度，而在詞關聯 性部份較傳統的編碼方式得到了改善，也避免了有多少個單詞就有多少個維度 的窘境，常見的Distributed Representations of Words Model 有Word2Vec 與 Glove。 Savigny [23]收集了10 部影片共8115 則評論且透過CNN 搭配三種Word Embedding 的方式比較每個評論的情緒類別，實驗結果可以看出Word2Vec 的正 確率較Glove 與Doc2Vec 還高，所以本文採用Word2Vec 做字詞向量[24]， Word2Vec 是 Google 於 2013 年由 Tomas Mikolov 等人所提出，透過學習大 量沒有標籤的文本資料，將字詞用數學向量的方式來代表他們的語意。並將字 詞嵌入到一個空間後，讓語意相似的單字可以有較近的距離，可用來表示詞與 詞之間的語意關係。 Word2Vec 架構分為兩種，分別為 CBOW（Continuous Bag-of-words）與 Skip gram，CBOW 使用上下文作為 input layer 以預測中間詞，Skip-gram 則 以中間詞為 input layer 以預測其上下文。 圖 四、CBOW 與Skip gram 差異圖 根據 Tomas Mikolov 認為相較於 Skip-gram，CBOW 適合用於文章內容短 且訓練資料量大時，且訓練速度也較快，考慮到YouTube 文字特徵長度較短， 因此本論文 Word2Vec 模型使用 CBOW 作為訓練架構。
