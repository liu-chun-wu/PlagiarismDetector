我們將在本小節介紹卷積神經網路的訓練過程與步驟。 i. 輸入與輸出 為了觀察網路的泛化性，即其對於未知樣本的偵測能力，我們以 9:1 的比例將20 個類別，一共20000 張的樣本分為訓練與驗證集，也 就是分出18000 張為訓練集，2000 張為驗證集。每張訓練樣本均以 416416 解析度輸入網路，並迴歸出2D 偵測框與3D 偵測框，再以標 記的2D 邊界框與類別來計算第一次偵測的誤差，如公式 (3.10)；最 後則以標記的3D 邊界框與類別來計算第二次偵測結果的誤差，如公 式 (3.5)，最後將兩次輸出的誤差加總來引導網路學習。 ii. 網路的訓練策略 每個實驗進行200 回合 (epoch) 的訓練，而迭代訓練所使用的優 化器為 Adam [41]，我們以學習率衰減 (learning rate decay) 策略搭配 Adam 來使訓練效果更好。相關的衰減策略有餘弦 (cosine) 學習率衰 減法、階層 (step) 學習率衰減法、指數 (exponential) 學習率衰減法 - 43 - 等等，而我們採用的學習率衰減策略為 Pytorch 所提供的 ReduceLROnPlateau 函式，該函式會針對我們所給定的值進行監測， 該監測值若在k 個回合數內都無法優化，函式會將學習率乘上n 倍來 協助網路學習，並在調整完學習率後經m 個回合後再重新開始監測 數值；我們對每回合的訓練誤差的平均值進行監測，並將k 設為3， n 設為0.6，m 設為2，初始學習率設為0.001。實驗中我們固定批次 大小 (batch size) 為12 來訓練網路，即每次迭代訓練時會取12 筆訓 練樣本進行預測後計算誤差並修正模型權重。故總訓練樣本為 20000 筆時，一個訓練週期需要經過1667 次迭代訓練。當總訓練週期為200 時，需要333400 次迭代訓練，其學習率變化方式如圖 4.3 所示，縱 軸為學習率，橫軸為回合數。 圖 4.3. ReduceLROnPlateau 學習策略。 iii. 卷積神經網路調整 為了讓網路擁有更好的效果，我們調整感興趣區塊卷積核的採樣 點數量，以77、1111、及1313 三種不同的感興趣區塊卷積核來 比較實驗結果。另外對原始深度分支網路做三種調整，第一種為以雙 0.00002 0.00004 0.00006 0.00008 0.0001 - 44 - 線性內插將原始深度影像降維至各輸出層的解析度，再將降維後的資 料並聯 (concatenate) 骨幹特徵，最後直接以感興趣區塊卷積提取特 徵，如圖 4.4 紅色線段所示。 圖 4.4. 以雙線性內插降維的原始深度影像分支網路 (紅色線段)。 第二種是全程均以步伐 (stride) 為2 的33 卷積將原始深度影像 降維至各輸出層的解析度，將所得到的特徵並聯骨幹特徵，最後交由 感興趣區塊卷積提取特徵，如 圖 4.5 所示。 - 45 - 圖 4.5. 全程均以步伐為2 的33 卷積提取特徵的原始深度分支網路 (紅 色線段)。 第三種與第二種一樣，都是全程以步伐為2 的33 卷積，不同之 處在於並聯骨幹特徵後會再做一次33 卷積，再交由感興趣區塊卷積 提取特徵，如圖 3.8 紅色線段所示。
