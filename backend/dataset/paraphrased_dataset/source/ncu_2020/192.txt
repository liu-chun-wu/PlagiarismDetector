中文文本是由一個或數個句子以上所構成，句子則由字與詞彙所組成，而詞 彙可說是具有語意最小的語言單位也是分析方法最重要的部份。英文在進行文本 分詞時，可依據空格可將英文文件切割成一個個獨立的單字(word)，在處理分詞 上較為容易且可根據詞幹進行調整；中文不像英文單字在字與字有空間可做為分 詞依據，中文也無詞幹可參考且單字與單字間無空間，故需要先進行分詞而且分 需考量語意來切割成詞彙。 將上一步3.2資料清洗後的資料集進行相關的中文詞性分詞(斷詞)、特徵詞提 取、建立關鍵詞庫語料等整理。使用自行開發的詞性分類工具(Python＋CKIP套 件)，將預處理後的5845筆資料集使用中研院CKIP小組所研發的分詞(斷詞)技術 進行中文斷詞。 預測客訴問題分類模型：使用本文探勘技術於Google play 商店客訴留言 透過工具(自行開發Python程式＋CKIP套件)我們針對本文探勘的關鍵欄位 (New Review Text)的留言內容的進行中文斷詞。首先將欄位內的留言內容使用自 行開發Python程式＋CKIP套件來進行斷詞，接者將自行開發Python程式在執行斷 詞後輸出的*.CSV文檔透過Orange軟體中的文字探勘套件工具，將這些各個不同 詞性(part of speech)例如：名詞(N)、動詞(V)、連接詞(C)、副詞(ADV)、介係詞(P)、 語助詞(T)、感嘆詞(I)等詞性特徵詞彙繪製出特徵詞彙文字雲(Word Cloud)以及不 同詞性的特徵詞彙語料集，共7個主類別詞性以及35種次類別詞性(相關詞性說明 可參考附錄表12-CKIP詞性列表說明)，接著針對這些帶有詞性的特徵詞彙透過 Orange軟體中的文字探勘套件工具中的文字預處理(Preprocess Text)以及文字雲 (Word Cloud)將這些不同詞性的特徵詞彙語料集經由詞頻計算(TF)出各詞性中頻 率較高(≧10)的特徵詞彙建立(Report)成關鍵詞彙語料庫集Keywords.txt，並將這 些關鍵詞彙認定為有意義的詞彙，而頻率較低的詞彙表示這些詞彙目前提供的內 容信息較少則暫不列入詞庫中。接著再將這些提取出來各個詞性中頻率較高的關 鍵詞彙做為自變數變項使用，如圖3： 圖 3 關鍵詞彙清單 本文詞性特徵詞提取-自行開發Python程式碼範例： pipeline = CkipPipeline() strPosN = ["A","Caa","Cab","Cba","Cbb","Da","Dfa","Dfb","Di","Dk","D","Na","Nb","Nc","N cd","Nd","Neu","Nes","Nep","Neqa","Neqb","Nf","Ng","Nh","Nv"] 預測客訴問題分類模型：使用本文探勘技術於Google play 商店客訴留言 strPosV = ["I","P","T","VA","VAC","VB","VC","VCL","VD","VE","VF","VG","VH","VHC"," VI","VJ","VK","VL","V_2"]
