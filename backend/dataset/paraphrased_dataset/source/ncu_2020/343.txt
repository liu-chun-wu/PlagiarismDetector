原架構 9DoF SE-YOLO 為一階段偵測網路，由於要將對齊偵測模組 (align detection model, ADM) 加入原架構，使網路能先提出能對預訂的錨 框個別進行大小及位置的調整，再以錨框內的特徵迴歸出更準確的結果， 因此我們提出的架構會有兩階段偵測和一次的輸出。 第一次偵測為針對影像平面的物件偵測，以cx, cy 表示影像左上角至 網格的水平及垂直距離，σ為 sigmoid 函數，tx, ty 為預測物件的中心點座 - 32 - 標，並在每個網格中預先定義一種長寬比的錨框 (anchor box) pw, ph，錨 框的設置方式如同 YOLOv3 是以離線的 K-means 類聚訓練集的物件真 實數據 (ground truth) 的長寬比，但不同的是YOLOv3 設置五種不同長寬 比的錨框。而tw、th 為預測框之長寬大小比例，及是否包含物體的置信度 to (confidence)，預測值對應於邊界框值bx、by、bw、bh，如圖 3.9 所示， 我們將預測的值用以調整所預定的錨框，得到學習到的錨框 (learned anchor)。 圖 3.9. 第一次偵測的錨框與學習到的錨框之關係。 第一次偵測出的張量格式為預測之物件在圖像平面的資訊，包括表 示預測框中心點的平移分量x, y，及預測框的寬高w, h，如圖 3.10 所示。 - 33 - 圖 3.10. 第一次偵測所得到的資料之張量格式。 第二次偵測為針對物件偵測的位置與類別與原架構相同，為針對3D 空間的物件偵測。以cx, cy 表示影像左上角至網格的水平及垂直距離，σ 為 sigmoid 函數，tx, ty 為預測物件的中心點座標，tz 為預測之深度，pw, ph, pl 為在每個網格中設有的錨框之長寬高，tw, th, tl 為預測框之長寬高大小比 例， qt , 1qt , qt , qt 為以四元數表示的預測框之旋轉姿態，預測值對應於 邊界框值bx, by, bz, bw, bh, bl, qb , 1qb , qb , 3qb ，如圖 3.2 所示。 第二次偵測輸出的張量格式為預測的物件在3D 空間下的9DoF 資 訊，包括表示預測框中心點的平移分量x, y, z，預測框姿態的四元數旋轉 分量q0, q1, q2, q3，此外預測物件在真實世界下的長寬高w, h, l，如圖 3.3 所示。 - 34 -
