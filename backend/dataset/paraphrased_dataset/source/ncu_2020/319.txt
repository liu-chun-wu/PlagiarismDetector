研究基於公式(3)去做改進，原公式結合詞語頻率、句子位置、句子長度、標題，去 提取包含新聞關鍵事件的的主題句子(Wang, Zhao, & Wang, 2010)。 𝑆𝑆𝑖= 𝛼 𝑓(∑ 𝑡𝑓(𝑤) ⋅𝑖𝑑𝑓(𝑤) 𝑤∈𝑆𝑖 ) 𝐿𝑒𝑛𝑔𝑡ℎ𝑖 + 𝛽𝑓(𝐿𝑒𝑛𝑔𝑡ℎ𝑖) + 𝛾𝑓(Position𝑖) + 𝛿 𝑓(∑ ∑ 𝑤∈𝑇 𝑤∈𝑆𝑖 ) |𝑇| ⋅Length𝑖 (3) α, β, γ, δ 是計算詞頻、句子位置、句子長度、標題詞重疊率的權重，詞頻採用TF- IDF，𝑆𝑖為新聞中的一段句子，𝑇是新聞中的標題，f(x)是每個部分的正規化，𝑓(𝑥) = 𝑥/(∑ 𝑥𝑖 𝑆𝑖∈𝐶 )，C 是新聞的內容。句子依據計算出的𝑆𝑆𝑖分數作排序，取出N 個最好的句 子成為集合。 根據 (寧建飛、劉降珍, 2016)的研究，TextRank 結合Word2Vec 比起TF-IDF 在關鍵 詞提取準確率、召回率、F 值都有較好的表現，且TextRank 的計算同樣有考慮詞語頻率， 另外TF-IDF 較TextRank 依賴語料環境，若是領域及語料不適合，則會對結果造成影響， 另一方面TextRank 則較無此問題。故將詞頻的部分改成使用TextRank+Word2Vec 分數 去做計算，其餘部分則不變動，得到公式(4) 𝑆𝑆𝑖= 𝛼𝑓( ∑𝑇𝑒𝑥𝑡𝑅𝑎𝑛𝑘(w) 𝑤∈𝑆𝑖 ) + 𝛽𝑓(𝐿𝑒𝑛𝑔𝑡ℎ𝑖) + 𝛾𝑓(Position𝑖) + 𝛿 𝑓(∑ ∑ 𝑤∈𝑇 𝑤∈𝑆𝑖 ) |𝑇| ⋅Length𝑖 (4) 3.4.1. Word2Vec 模型訓練 Word2Vec 需要大量文字資料將字詞轉為向量，將字詞嵌入到一個空間後，讓語意 相似的單字可以有較近的距離，但在將資料丟入此模型運算前，需要做一些資料前處理。 訓練語料是中文維基百科，由於維基百科是開放資料可以直接下載，但是資料裡有 許多雜訊需要處理。使用WikiExtractor 抽取器抽取圖3.4 所示，這個工具可以自動提取 及清理語料。 圖 3.4 WikiExtractor 提取資料圖 抽取完後發現資料有簡體有繁體不是統一的，使用OpenCC 工具可以將龐大的資料 全部轉繁體或簡體，選擇轉為繁體後，獲得整理好的wiki 資料，需進行斷詞和去除停用 詞，方便機器更好訓練文本，完成前處理後，開始Word2Vec 的訓練。 此階段使用Gensimg 開源函式庫提供的Word2Vec 模型來進行，將預處理好的資料 放入Word2Vec，此階段任務為將資料訓練為詞向量。相關參數設置如下： Sentences：訓練的句子集，為處理好的維基百科資料。 Size=200：訓練出的詞向量會有200 維，語料足夠下較有區分度。 Sg=1：表示採用Skip-gram，sg=0 表示採用CBOW Window=10：上下文取幾個字。 min_count=64：若小於這個詞出現的次數，那它就不會被視為訓練對象。 3.4.2. 提取主題句子 3.4.2.1. TextRank 分數 訓練好Word2Vec 後，開始使用TextRank 演算法計算新聞本文的句子權重，將傳統 TextRank 計算詞語相似度的公式(5)進行改進。 Similarity(𝑆𝑖, 𝑆𝑗) = |{𝑤𝑘│𝑤𝑘∈𝑆𝑖&𝑤𝑘∈𝑆𝑗}| log(|𝑆𝑖|) + log(|𝑆𝑗|) (5) 原算式為利用兩個句子之間相同的詞語個數來計算相似度，在有了Word2Vec 訓練 出來的詞向量後，可透過計算兩個詞向量之間的餘弦距離，來獲得詞語的相似度，如公 式(6)。 cos(𝜃) = 𝐴⋅𝐵 ∥𝐴∥∥𝐵∥ (6) 兩個向量之間的角度的餘弦值可確定兩個向量是否指向相同的方向。兩個向量有相 同的指向時，餘弦相似度的值為1；兩個向量夾角為90°時，餘弦相似度的值為0；兩個 向量指向完全相反的方向時，餘弦相似度的值為-1。 計算句子之間相似度後，可利用TextRank 公式(7)計算句子的權重： WS(𝑉𝑖) = (1 −d) + d × ∑ 𝜔𝑗𝑖 ∑ 𝜔𝑗𝑘 𝑉𝑘∈𝑂𝑢𝑡(𝑉𝑗) 𝑉𝑗∈𝐼𝑛(𝑉𝑖) 𝑊𝑆(𝑉𝑗) (7) 即可得到TextRank 分數，在依照公式進行標準化，將得到的句子分數除以所有句子 分數的總和，即得到最終每個句子的TextRank 正規化分數，原公式(3)中有將TF-IDF 分 數除以句子長度，以避免句子字數的多寡影響TF-IDF 分數，但由於TextRank 分數的計 算過程中是考慮句子的相似度、分數計算也有將長度納入考量，故將長度由分母移除。 3.4.2.2. 句子長度分數 一般來說句子越長，越可能包含越多的重要資訊，較長的句子為關鍵句的可能性也 因此較高，研究以字數長度作為分數，在除以總字數做正規化，例如一篇500 字的文章 有一個25 字的句子，經運算後得到的長度分數為25/500 也就是0.05。 3.4.2.3. 句子位置分數 重要的句子常常在文章段落「很早」或「很晚」出現，而新聞常使用的倒金字塔結 構中越重要的資訊通常越早出現，也因此加入了句子位置的分數去做計算(Edmundson, 1969)。 本研究根據文章段落做區分，在前面段落的句子會有較高的分數，並依此向後面遞 減，起始分數設置為0.1，遞減分數設置為起始分數/段落數，例如文章段落數為4 的狀 況下，遞減分數為0.025，第一個段落分數為0.075、第二個段落為0.05，同一個段落的 句子會有相同的位置分數，可得到所有句子的位置分數，最後將每一個句子位置分數除 以總位置分數，可得到正規化的句子位置分數。 3.4.2.4. 標題重疊率分數 新聞的標題常常會包含事件的基本要素資訊，例如人物、事件、地點，如果要抽取 關鍵句可將標題當作參考，去計算標題的重疊率分數(Paice, 1990)。 計算方式為句子與標題共同擁有的詞數量，假如新聞標題與其中一個句子共同擁有 的詞為「流浪貓」、「棄養」，則此句子的重疊率為2，在將每一個句子的重疊率分數加總， 並將每一個句子的重疊率分數除以總分數，得到正規化的標題重疊率分數。 3.4.2.5. 公式總分計算 最後在將四個計算好且正規化後的分數：TextRank 分數、句子位置分數、句子長度 分數、標題重疊率分數乘上各自的權重α、β、γ、δ，可得到一個句子總分，從中取出N 個最好的句子成為主題句子集合，可包含新聞的關鍵事件。
