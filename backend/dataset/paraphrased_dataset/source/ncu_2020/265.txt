傳統的長短期記憶模型無法針對需要關注的部分進行語意的提取，為了 能夠針對需要關注的部分進行語意的提取，Li Zhao 接著提出長短期記憶結 合注意力模型(Attention-based LSTM, AT-LSTM)，此模型將觀點詞向量放在 長短期記憶模型每個隱藏層的輸出後面，將兩者連接後執行關注的動作，找 出各隱藏層輸出的語意占比，並合成為語意代表向量。AT-LSTM 的運算式 如公式(8)~(10)所示： M = tanh ([ 𝑊ℎ𝐻 𝑊𝑣𝑣𝑎 𝑒𝑁]) (8) α = softmax(𝑤𝑇M) (9) r = H𝛼𝑇 (10) 在上述公式中，H 表示隱藏層的輸出，va 表示觀點詞向量，運算符號在 公式(8)表示：𝑣𝑎 𝑒𝑁= [v; v; … ; v]，α為向量表示注意力權重占比，r 為給 定觀點詞彙下語句的加權表示，Wh、Wv 為調整矩陣。AT-LSTM 的架構如 圖 3.4 所示，圖中的N 為語句中詞語的總數，w 為各詞語的詞向量，h 為 長短期記憶模型在各個隱藏層的輸出向量，va 為觀點詞向量，α為各隱藏 層輸出結果所占的合成比例。 圖 3.4 觀點嵌入之長短期記憶模型架構圖(Y. Wang et al., 2016)
