由於本研究的目的在產生新聞導言與摘要任務相當類似，只是導言的分類和手法比 起摘要更加多樣，故就以下的研究做討論。 自動摘要有許多的用途，包括新聞標題、電影預告介紹、會議記錄、筆記大綱等等， 但是過往卻無中文新聞導言的相關研究 (Brownlee, 2017)。 自動摘要方法主要有兩大類，生成式摘要(Abstractive Summarization)和抽取式摘要 (Extractive Summarization)，抽取式摘要從文章中挑選出重要、富含資訊的句子，並集合 起來當作摘要，而生成式摘要指的是機器理解文章內容後，重新以自己的詞語闡釋句子， 目的為產生更為邏輯通順的摘要 (Dalal & Zaveri, 2011; Das & Martins, 2007; Gupta & Lehal, 2010; Mani & Maybury, 1999; See, Liu, & Manning, 2017)。 摘要可分為多文檔和單文檔摘要，單文檔摘要為單篇文章產生一個摘要，多文檔摘 要為理解多篇文章後產生一個摘要，由於多文檔摘要與本研究較無關，故不討論。而單 文檔摘要重點在於文章的重要資訊評估，也就是要讓機器去理解文章中的哪個部分為重 點資訊。 本研究主要使用抽取式摘要常用的關鍵詞、句子權重計算，去提取出新聞重要的關 鍵事件句。 2.3.1. 抽取式摘要 抽取式摘要是透過模型或演算法取得句子或詞語權重，然後根據權重，去原文中尋 找出跟重點資訊最接近句子。本研究利用抽取式摘要的核心方法從文章中抽取出關鍵性 的句子，再分析出5w1h 事件，故介紹抽取式相關的技術。 2.3.1.1. 詞頻-逆向檔案頻率 詞頻-逆向檔案頻率(Term frequency-inverse document frequency, TF-IDF) 是一種統計 方法，常常使用在資訊檢索與文字挖掘方面，透過加權技術來評估個別詞語對於檔案集 或語料庫的重要程度，主要分為詞語頻率（Term frequency, TF）以及逆向檔頻率（Inverse document frequency, IDF）。詞頻是詞語在文章中出現的次數越多則重要性越高，逆向文 件頻率是指若是某個詞語在很多文章都出現過，代表它可能不是這麼重要的詞語，而是 常用的動詞連詞，它的IDF 就會比較小，(Ramos, 2003)。TF-IDF 經過訓練後，可以找出 一篇文章內出現頻率高，但在大部分的文章中出現頻率卻很低的詞語，也代表該詞語的 TF-IDF 值相當的高，可以做為該文章的代表也就是關鍵詞，也因此TF-IDF 有著保留重 要的詞語、過濾掉常見的詞語的功能。 TF-IDF 的優點為演算法簡單快速，假如語料環境質量好，在統計上較有優勢；缺點 是很依賴語料環境的支援，需要大量質量高的語料去訓練，且對於出現在文章各處的詞 語都一視同仁，而通常標題或是文章首尾的詞語通常較為重要，這方面的權重需要去調 整。 2.3.1.2. 文章權重演演算法 文章權重演演算法(TextRank) (Mihalcea & Tarau, 2004) 是從網頁重要性排序法 PageRank 演算法 (Brin & Page, 1998)遷移而來，PageRank 根據全球資訊網(WWW)網頁 之間的連接關係計算每個頁面的重要性；TextRank 則將詞視為網頁上的節點，根據詞之 間的共同出現關係，以下簡稱共現關係，來計算每個詞的重要性。 PageRank 的核心思想如下： 1. 一個重要的網頁，應該會被很多網頁連接到，反過來說被很多網頁連接到的網頁， 也應該會很重要，因此該網頁的PageRank 值會較高。 2. 如果一個重要，也就是PageRank 值較高的網頁連接到其他網頁，被連接到的網頁 PageRank 值也會提高。 所以TextRank 也改自PageRank 的核心思想，區別在於：PageRank 演算法根據網頁 之間的連接關係構造網絡，而TextRank 根據詞之間的共現關係構造網絡，TextRank 的 核心公式(1)如下： (Mihalcea & Tarau, 2004) 𝑊𝑆(𝑉𝑖) = (1 −𝑑) + 𝑑× ∑ 𝜔𝑗𝑖 ∑ 𝜔𝑗𝑘 𝑉𝑘∈𝑂𝑢𝑡(𝑉𝑗) 𝑉𝑗∈𝐼𝑛(𝑉𝑖) 𝑊𝑆(𝑉𝑗) (1) 𝑉𝑖、𝑉𝑗：為節點也就是句子。 WS(𝑉𝑖)、WS(𝑉𝑗)： 一個節點的權重(weight sum)，TextRank 分數。 d：阻尼係數，代表某一節點指向其他節點的概率，一般為0.85。 𝜔𝑗𝑖：兩個節點之間的相似度 根據公式(1)，可以拆成兩個部分，d 被固定的狀況，前部分是單純的常數，而後半 的部分則可以得到相鄰節點對句子的貢獻程度，若是相鄰節點越重要，則分數會越高， 根據節點i 與j 的權重占比，再乘上節點j 的權重分數來得到。 在依照上面的公式反覆運算，並更新節點的TextRank 數值WS(𝑉𝑖)直到收斂後，就 能得到整篇文章的句子權重分佈。 而𝜔𝑗𝑖句子相似度的計算方式，其相似度計算公式(2)如下： Similarity(𝑆𝑖, 𝑆𝑗) = |{𝑤𝑘│𝑤𝑘∈𝑆𝑖&𝑤𝑘∈𝑆𝑗}| log(|𝑆𝑖|) + log(|𝑆𝑗|) (2) 𝑆𝑖和𝑆𝑗代表要比較相似度的句子，𝑤𝑘代表句中第k 個詞語，{𝑤𝑘|𝑤𝑘∈𝑆𝑖&𝑤𝑘∈𝑆𝑗}代 表同時出現在𝑆𝑖和𝑆𝑗中的詞語，|𝑆𝑖|和|𝑆𝑗|代表句中詞語的個數。根據此公式可以求出兩 個句子之間的相似度。 2.3.1.3. 詞向量模型 Mikolov 等人在2013 年提出在自然語言處理有重大意義的詞向量模型Word2Vec， 此模型透過學習大量文本資料，將字詞用數學向量的方式來表示語意。並將字詞嵌入到 空間後，讓語意相似的單字可以有較近的距離 (Mikolov, Chen, Corrado, & Dean, 2013)。 Word2Vec 的核心概念就是一個詞的語意能夠被其他鄰近一起出現的詞所定義，每 個詞一定距離內的上下文，這個距離被稱為windows size。Word2Vec 透過訓練，讓模型 去預測一個詞附近的上下文來建立向量，若是較為類似的上下文章，就會產生較為接近 的詞向量。 Word2Vec 的訓練步驟如下： 1. 需要大量的語料，例如維基百科。 2. 每個字詞用一個向量來表示 機器沒有辦法直接辨識文字，需要轉換成向量的方式讓機器去做數學運算，通常是 使用離散(one hot vector)方式去表示，例如apple 用[1，0，0，0，0]、banana 用[0，1， 0，0，0]表示。 3. 反覆運算所有的位置t 的詞，t 包含一個中心詞c 和一群上下文o 圖 2.1 Window size 示意圖 (Hsu, 2020) 如圖2.1，一開始into 為中心詞c，window size 的值為2，problems、turning、banking,、 crises 就是上下文o。而當t=t+1 時，中心詞會從into 變為banking，而此時turning、into、 crises、as 就是上下文o。 4. 用詞向量的相似度來計算 Word2Vec 分為兩個主要模型，CBOW 與Skip-gram。CBOW 模型用上下文預測中 心詞，Skip-gram 模型反過來利用中心詞去預測上下文，如圖2.2 圖 二.2 Word2Vec 的兩種模型：CBOW 與Skip-gram (Mikolov et al., 2013) 在有了Word2Vec 後，許多研究也透過Word2Vec 生成詞向量，再利用TextRank 尋 找文章的中心句形成摘要 (寧建飛、劉降珍, 2016; 顧益軍、夏天, 2014)，傳統的TextRank 是用相同單詞個數來計算相似性，卻沒有考慮單詞的詞性、相義詞、反義詞等許多因素， 比如說：「貓」、「狗」兩個詞在傳統的TextRank 上相似性為零，而在大眾認知上，它們 其實有一定的關係，比如說都是哺乳類也是人類常養的寵物，但傳統的TextRank 無法 計算，因此此種計算相似性的方式並不優秀，而為改善此問題，許多研究開始利用 Word2Vec 來計算詞語的相似性 (Wolf, Hanani, Bar, & Dershowitz, 2014; Zhang, Xu, Su, & Xu, 2015)，來提升找到關鍵句的準確度。
