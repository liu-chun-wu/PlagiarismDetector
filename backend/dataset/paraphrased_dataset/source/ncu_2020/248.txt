詞嵌入(Word-embedding)是近年在自然語言處理中被廣泛使用的一種技 術。目的主要是想使用一個向量來表示每一個詞(Vector Representation)，如 此一來，就能把一段由許多詞組成的語句，轉換成一串由實數組成的向量來 表示，並把這樣數值化的資料，送到模型裡做後續的應用。早期模型包括詞 袋模型(Bag-of-Word)(Zhang, Jin, & Zhou, 2010) 以及一位有效編碼嵌入 (One-Hot-Encoding)。詞袋模型將文件中出現的詞彙，想像是放在袋子裡零 散而獨立的物件，如此一個袋子代表一篇文件，每個詞彙都是獨立的單位， 不考慮其相依性，此種獨立性假設簡化了很多文件自動處理的計算，因而廣 被採用。一位有效編碼嵌入統計文件中出現的詞彙，依據詞彙的數量建立該 數量維度的向量，這個向量在代表每個詞彙時，只有一個維度為 1 其餘為 0，詞彙之間也不具有相依性。上述兩種詞嵌入方法皆會碰到以下問題，隨 著詞彙的增加，文件的向量表示也會增加，每個文件可以包含詞彙表中很少 的已知單詞，導致稀疏性問題。此外，方法皆丟棄單詞順序忽略了上下文， 而忽略了文件中單詞的語義，讓許多自然語言處理的準確率下降。潛在語言 學(Latent Semantic Analysis, LSA)(Deerwester, Dumais, Furnas, Landauer, & Harshman, 1990)在1990 年由S.Deerwester 提出，用大量的文本進行統計分 析，並且通過奇異值分解（singular value decomposition, SVD）與維度約化 （dimension reduction）的處理，使語意空間中的雜訊能夠降低甚至去除， 進而讓詞彙間隱含的語意關係得到最精確的重現。後續研究提出Word2Vec 模型(Mikolov, Sutskever, Chen, Corrado, & Dean, 2013)以及全局的詞向量表 示(Global Vectors for Word Representation, GloVe)模型(Pennington, Socher, & Manning, 2014)。 Word2Vec 透過學習大量文本資料，將字詞用數學向量的方式來代表語 意。並將字詞嵌入到一個空間後，讓語意相似的單字可以有較近的距離。模 型在學習時不是讀完一次大量文本就得到最後的詞向量，而是迭代的讀了 一次又一次，逐漸將模型中的詞向量更新，以趨近最佳結果。模型中又可細 分為連續式詞袋模型( Continuous Bag-of-Words Model, CBOW )(Mikolov, Chen, Corrado, & Dean, 2013)和連續式跳躍模型( Continuous Skip-Gram Model, SG )(Mikolov, Sutskever, et al., 2013)兩種方式訓練。CBOW 透過上下 文來預測中間的目標詞，而SG 是希望利用中間的詞去預測上下文，如圖
