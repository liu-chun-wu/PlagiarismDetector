由於網路多了一次偵測，因此必須加入第一次偵測之損失函數。而第 一次的偵測為針對影像平面的偵測，因此我們採用 YOLOv3 的損失函數 來引導網路學習；由於第二次偵測為三維空間偵測結果，因此我們採用原 架構 9DoF SE-YOLO 的損失函數，並不進行修改，如公式 (3.5)。因此整 體損失函數修改後為， _ DoF SE YOLO yolov    L L L (3.9) 在影像平面的偵測著重於使新錨框能夠框出可能存在物件的位置， 其目的是為了幫助得到更精確的物件特徵，因此我們希望學習到的錨框 (learned anchor) 的中心點偏移位置及大小可以更精確。有鑑於此，我們 將學習到的錨框中心點偏移位置及大小損失的權重λcoord 設為2， - 38 - ˆ ˆ 1 [( ) ( ) ] ˆ ˆ 1 [( ) ( ) ] ˆ 1 [ ln ] ˆ (1 )ln(1 )] S obj ij x x y y yolov coord i S obj ij w w coord h h i S obj ij ij ij coord i S noobj ij ij ij noobj i t t t t t t t t C C C C                         L ˆ [ ln ] S classes obj ij coord k k i k c c       (3.10) 其中， coord  ：常數設為2， noobj  ：常數設為1， S ：網格總數， obj ij ：表示在方格i 中的第jth 個邊界框是有回應的， ij C ：表示在方格i 中的第jth 個邊界框有物件的機率， kc ：表示在方格i 中的第jth 個邊界框有物件，且屬於kth 類別的機率值。
