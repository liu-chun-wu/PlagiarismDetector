本研究屬於監督式學習的預測建模，使用目前使用成長快速之Python 程式 語言所寫的機器學習開源軟體模組套件，如表3-4 所示[18]，ANN 另加入使用 Keras 深度學習模組，機器學習模型的程式開發介面為Jupyter Notebook 及版 本資訊如圖3-6 所示。 Using GridSearchCV to get best hyperparameters Evaluated by MAE value Select the best model group  (RMSE + MAE ) SD15 , SD16         SD16 , SD17        SD17 , SD18 Best Model Model Comparison (RMSE + MAE ) Using best hyperparameters on best model group 表3- 4  Python 程式之機器學習模組套件 （圖片資料來源： [18]） 圖3- 6  Python 及Jupyter Notebook 版本資訊 線性回歸 Linear Regression 最基本的回歸方法是易於解釋的線性回歸模型，其假設變數之間存在著近似 線性的關係，其主要焦點是在於模型的偏差（Model bias），然而脊回歸（Ridge regression）則是在於模型的方差（Model variance）[30]。線性回歸是一種能 預測回應值或目標值與預測值之兩個變數之間關係的靜態技術，即自變數(x)與 依變數(y)中之線性關係，簡單線性回歸（Simple linear regression）是使用 單一自變數來預測單一依變數，而一個以上的自變數則稱為多元線性回歸 （Multiple linear regression），其預測的品質是在於這兩個變數之間關係的 強弱[31]。 脊回歸 Ridge regression 在多元線性回歸分析當中常見的問題是輸入變數的共線（collapsibility） 或接近共線性（near–collinearity）而導致輸入變數有不適定問題（ill posed problem），為了規避此問題，已經開發出各種技術，其中之一是脊回歸[32]，也 稱為Tikhonov 正則化[33]。所謂的collapsibility 是指無論共變數是否存在於 模型裡，皆不會影響到預測變數（Predictor variable）與反應變數（Response variable）之間的關係，整體而言，當共線性存在於線性回歸模型中，並不一定 會對於回歸結果造成不好的影響。因此，當模型裡預測變數間存在共線性時，變 數是否從模型中移除必須謹慎思量[34]。脊回歸屬於改良的線性回歸分析。 決策樹 Decision tree 決策樹是分類與回歸分析中常用的模型，主要是因其有較佳的解釋性及易用 性，原因在於可轉換為一組規則，除此之外，其輸入能處理名義變數（nominal variable）和數值變數，並可與變數尺度（variable scale）無相關性。然而其 績效通常低於標準而被認為不是很可靠的模型[35]。決策樹的優點是可處理資料 有錯誤值及缺值，而大部份的決策樹模型例如ID3, C4.5 以及CART 都有貪婪特 性（Greedy characteristic ），因此又被稱為貪婪演算法，訓練模型時需要避 免樹過度生長而造成過度擬合（Over fit）的問題[36]。 隨機森林 Random Forest 隨機森林是樹預測器的組合，結構是擁有數棵決策樹的模型，在森林中建構 出個別獨立的決策樹，然後以投票方式以決定最後的結果。邏輯是每棵樹都依資 料屬離散或連續型取其眾數或是平均值。使用於回歸的隨機森林是根據隨機向量 (Random vector)生長樹來形成的，因著大數定律（Law of Large Numbers）防 止了過度擬合（Over fit），使得隨機森林成為有效率的預測工具[37]，所謂的 大數定律是依據的樣本數越多，平均值則會更接近期望值，是用來描述多次重複 實驗結果的定律。然而在過多的決策樹下，會使得計算的成本相對提高，這包含 了運算的時間與資料儲存空間的成本。基本上，隨機森林演算法中有兩個重要參 數分別是ntree 和mtry，ntree 是生長樹木的數量，為確保每個輸入實驗至少被 預測幾次而不應太小。mtry 是在每個分組中隨機抽取為候選變數的數量[38]。 由於過度擬合的問題在機器學習過程中很常見，尤其是加入過多的參數時就會發 生，若能包含更多的樹（ntree）時通常可以減少過度擬合，而當在每棵樹中包 含了過多的特性值（mtry）或允許每棵樹有太多的終端節點時也會導致過度擬合 [39]。 支援向量回歸 Support Vector Regression 在不同的銷售預測研究中，已應用了支援向量回歸來執行，例如電腦主機板 生產銷售、電腦代理商銷售預測、煙草銷售等[40] [41] [42]，當然也包括時尚 產品-女用包[17]，在時間序列中是核心的方法之一 [43]。 梯度提升 Gradient Boosting Regressor GBR 是屬於監督式學習模型，有許多可以調整的超參數（Hyper parameters） 及損失函數（Loss functions），如ls,lad,huber,quantile loss function， 因此比其他的模型更有彈性及預測更準確[44]。其使用損失函數做為評價模型的 效能，損失函數越小，效能越好，讓損失函式沿著梯度方向下降來使模型不斷改 進提升效能，即為提升算法（Boosting）。其能處理異質資料（Heterogeneous data），但模型訓練較費時，而執行預測時則迅速[45]，梯度提升演算法在Kaggle 競賽中經常被用來執行預測，在分類與回歸預測中績效非常好[46]。 極度梯度提升 XGBRegressor XGBRegressor 亦屬於監督式學習模型，同樣有許多可以調整的超參數及損 失函數，其在鋼筋混凝土板的衝剪抗力預測研究中，預測的績效比ANN 及Random Forest 更好[47]，在Kaggle 競賽中亦經常被使用在預測中。 輕量梯度提升 LightGBM 輕量梯度提升（Light boost based Gradient boosting machine，LightGBM） 是一個效率有很高的梯度提升樹，其架構包含了決策樹和提升算法（Boosting） [48]。與XGBoost model 最大的不同點在於其使用直方圖演算法 （Histogram-based algorithm）來加速訓練過程，以降低記憶體消耗及部署 leaf-wise（best-first）的策略來生長樹。直方圖演算法的邏輯是將每個特徵 的連續值轉換成離散值後，並將每個特徵值建立特徵值直方圖，不需要預先存儲 結果，能在轉換後保存其值，通常足以儲存8 位整數，並且內存消耗可以是減少 到原來的1/8。這種粗略的劃分不會減少模型的準確性，其在每日的水分蒸散量 預估研究中，預測績效比M5 Model Tree（M5Tree）及隨機森林更好[49]。與其 他靜態方法及機學習方法比較，是一個可以讓計算時間成本及準確度達到平衡的 方法，在手勢識別研究中，其績效是非常好的[50]。 類神經網路 Artificial Neural Network ANN 是由相互連接的處理單元（神經元）構建的一組工具，每個連接都具有 與之關聯的權重。 在這些網路中，神經元被組織在各層級中，每個神經元接收 一組輸入（來自於前一層的輸出），然後輸入該層的非線性加權總和後再輸出到 連接的下一層的神經元。這些加權連接使模型可以"學習"變數之間的關係。單個 前饋類神經網路之組成元素有輸入層、單一或數個隱藏層與輸出層。關於隱藏層 與輸出層中的神經元，將激活函數置於其輸入值中。每個連接都具有與之關聯的 權重[51]。ANN 模型結構如圖3-7 所示，神經網路通常只有一個，偶有兩個隱藏 層也得到了廣泛的使用，並且績效表現很好。然而隱藏層數的增加同時也增加了 計算時間和過度擬合的風險，這會導致樣本外的預測效果不佳。本研究使用了兩 個隱藏層。 圖3- 7  ANN 模型結構 （圖片來源: [52]） 第四章 研究結果
