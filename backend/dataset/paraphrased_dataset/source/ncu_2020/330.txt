本節分為兩個部份，包含3D物件偵測及姿態估計資料集、修改 9DoF SE-YOLO [20] 架構之9D 物件姿態估計。 i. 3D 物件偵測及姿態估計資料集 我們使用 NVIDIA 提供稱為墜落的物品 (Falling Things) [22] 做為訓 練資料集，內容包括RGB 和D 兩種影像，相機內部參數 (intrinsic parameter)，標記資料的內容為物品的類別、3D 邊界框與姿態，資料集 共包括20 種家庭廚房物品。 ii. 9D 物件姿態估計 a. 輸入 RGB-D 影像：將對應的 RGB-D 影像以批次 (batch) 方式輸入 卷積神經網路中訓練。 b. Darknet-53 提取特徵：融合 RGB-D 影像資訊並提取特徵。 c. 預測2D 物件的位置、大小及類別：由骨架特徵 (backbone feature) 進行卷積並預測出物件在圖像平面中的2D 位置、大小、及類別，並 使用該資訊調整錨框。 d. 提取D 影像特徵：為了取得更佳的空間資訊，取出原始D 影像後對 其卷積數次以降維並提取有用特徵。 e. 並聯 (concatenation) 特徵：將骨幹特徵與D 影像提取出來的特徵並 聯在一起，後交由感興趣區域卷積 (RoI convolution) [23] 提取RoI 的 特徵。 f. 以感興趣區塊卷積進行特徵對齊並提取特徵：將先前調整後的錨框 之位置及大小資訊與並聯後的特徵作為感興趣區域卷積的輸入，以 - 4 - 此操作來達到特徵對齊並提取特徵。 g. 預測物件 9DoF 及類別：對感興趣區塊卷積提取出來的特徵再進行 數次11 卷積迴歸出物體在現實世界的大小、位置、姿態及類別。 h. 非最大值抑制 (non-maximum suppression) [24] 排除多餘預測結果： 對同一物件在相同位置可能偵測出多個邊界框，只保留最佳邊界框 結果並去除其他較差結果。 圖 1.1. 系統架構流程圖。 - 5 -
