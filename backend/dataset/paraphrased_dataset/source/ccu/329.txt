Apache Spark 是開放原始碼的叢集運算工具，允許用戶將資料戴入至記憶體內進 行儲存及運算。其核心是RDD(Resilient Distribution Dataset)彈性分散式資料 集，可與外部系統相容，例如HDFS。  Hadoop 和Apache Spark 兩者存在的目的不 盡相同。 Hadoop 是一個分散式資料基礎框架: 它將巨大的資料分派到由一般等級電 腦所組成的叢集中的多個節點進行儲存並管理，而不需要購買和維護昂貴的伺服器就 能達成高效能的運算環境。同時提供了MapReduce 的資料處理功能來完成分散式資料 處理。Spark 則是一個專門用來對分散式資料做大數據運算處理的工具，它並不負責 資料的儲存。由於Spark 沒有提供文件管理系統，所以，它必須和其他的分散式資料 管理系統進行整合才能運作。例如Hadoop 的HDFS 或其他的雲端的資料平台。但普遍 來說還是認為Hadoop + Spark 的結合是最好的。 Spark 因為其處理數據的方式不一樣，會比MapReduce 快上很多。Spark 在計算 時是將中間產生的資料暫存在記憶體中，批次運算完成後再寫回檔案系統。相較於 MapReduce 則是分次將資料從檔案系統讀出處理後先將運算結果寫回檔案系統，再讀 出進行下一次的處理，再將結果寫回檔案系統。Spark 在記憶體中的數據處理速度最 多可快過MapReduce 近100 倍。其處理概念可以圖2.5-6 表示。 圖2.5-6 Spark 與MapReduce 的處理概念 資料來源:本研究整理
