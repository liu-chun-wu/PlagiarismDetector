支援向量機 (Support Vector Machine, SVM) ( Osuna, Freund, & Girosi, 1997)是一 種在統計學習基礎上發展起來的機器學習方法，其最大特點是根據Vapnik 結構風險 最小化原則 (Cortes & Vapnik, 1995)。它的基本模型是定義在特徵空間上的間隔最大 的線性分類器，在解決小樣本、非線性及高維度等問題上具有傳統的機器學習方法所 不具備的優勢。 SVM 想要解決以下的問題：找出一個超平面 (hyperplane)，使之將兩個不同的集 合分開。因為實際資料可能是屬於高維度的資料，而超平面意指在高維中的平面。以 二維的例子來說，如圖3.16 所示，我們希望能找出一條線能夠將黑點和白點分開， 而且我們還希望這條線距離這兩個集合的邊界 (margin) 越大越好，這樣我們才能夠 很明確的分辨這個點是屬於那個集合，否則在計算上容易因精度的問題而產生誤差。 多特徵自動化表情辨識系統 圖3.16 SVM 超平面示意圖 (資料來源：本研究整理之) 假設我們有一堆點集合 {𝑥𝑖, 𝑦𝑖}, 𝑖= 1, … , 𝑛       𝑥𝑖∈𝑅𝑑, 𝑦𝑢∈{1, −1} 我們希望能 找到一條直線 f (x) = 𝑤𝑇𝑥−𝑏 使所有 𝑦𝑖= −1 的點落在 f (x) < 0的這一邊，且使 所有𝑦𝑖= 1 的點落在 f (x) > 0 的這一邊，這樣我們就可以根據 f (x) 的正負號來區 分這個點是屬於這兩個集合之中的那一個。我們把這樣的超平面稱為分離超平面 (separating hyperplane)，而距離兩邊邊界最大的就稱為最佳分離超平面(optimal separating hyperplane)，而支持超平面 (support Hyperplane) 是指與最佳分離超平面平 行，並且最靠近兩邊的超平面。 多特徵自動化表情辨識系統 第四章 實驗結果與討論
