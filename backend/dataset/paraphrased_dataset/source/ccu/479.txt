為了比較本研究中各種分類技術產生的預測模型之效能及準確率，本研究將所有 整理的資料分別建立訓練資料集與測試資料集，再利用10 摺交叉驗證法來進行各種 預測模型的資料驗證。 10 摺交叉驗證法是採用10 個交疊，目的為有效降低偏差與變異，此驗證方式是 將將原始資料集隨機分為十組，但互不重疊的子資料集，而每一組資料的類別都與原 始全部樣本的類別相近，運用其中9 組作為訓練資料(Training Set)，剩餘1 組作為效 能的測試資料(Validation Set)，以得到所需的評估數據。意即在訓練資料以分類技術 建構出預測模型後，再使用測試資料來驗證模型的準確率，這樣即為一摺資料組；之 後依序將其 9 組進行測試共10 次分類模型進行效能評估，最後分別計算出10 組預 測模型所對應的準確性，再取其平均值來做為整體效能評估的依據。運用此法可以克 服因為只抽樣1 次建立的預測模型可能導致的資料不均勻，如此得到的評估數據較合 理，可減少過適的可能性。 為評估各個預測模型之績效指標，本研究將以混亂矩陣(Confusion matrix)進行模 型效能評估。混亂矩陣是一種模型評估的工具，特別針對監督式學習的分類演算法， 藉由將模型預測的數據與測試數據進行比對，使用精準度(Precision)、召回率(Recall)、 F-measure 等指標對模型的分類效果進行評估，見表 6 所示。 智慧型判讀疾病分類碼：使用文字探勘於出院病歷摘要 表 6  混亂矩陣 實際             預測 Positive Negative Positive TP(True Positive) FN(False Negative) Negative FP(False Positive) TN(True Negative) TP+FP FN+TN  召回率(Recall):表示模型正確預測Positive 佔模型預測為Positive 的比率，可視為 Positive 預測的準確率。 Recall = 正確預測 Positive 的個數/預測 Positive 的總個數 = 𝑇𝑃 𝑇𝑃+𝐹𝑁  精準度(Precision):表示模型正確識別 Positive 的比率。 Precision = 正確預測 Positive 的個數/實際 Positive 的總個數 = 𝑇𝑃 𝑇𝑃+𝐹𝑃  F-Measure：是precision 和recall 的調和平均數，也就是取得兩者的平衡。 其通過以下等式計算： 𝐹𝛽= (1 + 𝛽2) ∗ 𝑝𝑟𝑒𝑐𝑠𝑖𝑜𝑛∗𝑟𝑒𝑐𝑎𝑙𝑙 (𝛽2 ∗𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛) + 𝑟𝑒𝑐𝑎𝑙𝑙 當belta=1 時等同於就是F-Measure，代表Precision 和Recall 都同等重要，如果 Precision 看得比較重要，那belta 就可以選擇小一點，當belta=0 時，F-Measure 就是 等於Precision；如果Recall 看得比較重要，那belta 就可以選擇大一點，當belta 無限 大時，F-Measure 就是Recall，本研究使用F-Measure 作為整體實驗評估指標。 智慧型判讀疾病分類碼：使用文字探勘於出院病歷摘要 第四章 實驗結果與分析
