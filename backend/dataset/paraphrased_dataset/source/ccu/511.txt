護理師在書寫護理記錄時，其所描述詞彙通常無法統一，例如：頭暈、暈眩，其 所代表的意思相同，但文字描述不同，而透過Word2Vec 可找到特定詞彙的相關字。 詞向量(Word Vector)為將詞彙轉換成向量，是為通過數值型資料取代文字，方便 後續作數學統計運算，以分析記錄文字所代表的意義。而Word2Vec 比較詞與詞之間 是否相似，為通過計算兩詞向量的餘弦相似度(Cosine Similarity)，其值範圍[-1,1]之間， 值越大代表越相關。 第三章研究方法 詞向量有很多種表達形式：One-Hot Representation 的向量表示會造成稀疏矩陣、 詞向量間無法表示關係並且字典量(Vocabulary)容易因資料集增長形成高維矩陣； Distributed Representation 的向量表示有多種形式Term-Document Matrix、Window Based Co-occurrence Matrix，一樣容易形成高維矩陣，通常需再透過SVD 降維，但其 訓練成本高，而Word2Vec 為較新的詞向量處理模型，具有高效的運算能力。 Mikolov, Chen, Corrado, and Dean (2013)所提出的Word2Vec 模型，有兩種架構 CBOW (Continuous Bag-of-Words)：根據上下文預測當前詞，Skip-gram：根據當前詞 預測上下文。其研究所述透過移除隱藏層並以霍夫曼樹(Huffman binary tree)結構來表 示字典(Vocabulary)、投影層讓所有詞共享並以向量求其平均等方式，使用結構較簡 單的模型，比起Neural Network，可以訓練出較好的詞向量，因計算複雜度較低，而 可從更大量的資料集中計算出準確的詞向量。 而實作過程，記錄樣本一樣需經過斷詞及排除停用字的前處理，再經由Word2Vec 建模後，通過前面章節所找到的關鍵字以此模型再找出其相關字。工具使用Python package gensim 3.5.0，於表 9 說明API 及參數。 表 9 Word2Vec API API： Word2Vec(sentences=None, sg=0, size=100, window=5, min_count=5, workers=3)： 訓練模型。 model.wv.most_similar (word, topn = 5)： 丟入測試資料，尋找相關字。 參數 說明 sentences 訓練資料，不同資料集因內容性質特性不同所找出的相關字會不一 樣。若納入各種資料來源，相關字結果應會較客觀；而若測試時要 找特定領域詞彙的相關字，則訓練資料亦需透過該特定領域資料集 來做訓練；較大的資料集利於訓練出更好的詞向量。 sg 模型架構，0：CBOW，1：Skip-gram Mikolov et al. (2013)提到CBOW 與Skip-gram 在語法準確性 (Syntactic Accuracy)差異不大；Skip-gram 比起CBOW 在語義準確 性(Semantic Accuracy)有很大的提升，但訓練速度較慢。 size 詞向量維度，較大size 需更多訓練樣本，但會得到更精確的詞向量。 window 一個詞的含意與其上下文相關，通過設定預測當前詞往前後看幾個 字來計算。 min_count 設定詞頻小於此數，則不被視為訓練對象，這些詞無足夠的訓練樣 本，排除可提升訓練速度。 workers 設定以多執行緒執行。 資料來源：整理自gensim 第三章研究方法
