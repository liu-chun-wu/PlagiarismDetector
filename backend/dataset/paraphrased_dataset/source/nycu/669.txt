Conditioned Generation Framework， Seq2Seq） 序列到序列條件生成模型（Sequence to Sequence Conditioned Generation Framework，以下簡稱Seq2Seq）[34]一種編碼器-解碼器架構（Encoder-Decoder Framework），主要用來解決某一類問題：輸入和輸出都是序列（Sequence）， 且輸入和輸出序列裡的組成順序、組成結構或序列長度不同的問題。Seq2Seq 有 廣泛的應用範圍，在自然語言處理的領域中，有機器翻譯[38][39]或是聊天機器 人[40]等多方應用。 圖13  Seq2Seq 示意圖 圖片來源：本研究製作 圖片說明：Sequence to Sequence 架構示意圖 Seq2Seq 透過一編碼器（Encoder）將一個輸入序列映射成某個定長的表徵 向量c，再透過解碼器（Decoder）將表徵向量c還原回輸出向量。由於輸入和輸 第24 頁 出都是序列資料，編碼器和解碼器通常皆各由循環神經網路（RNN）所構成，其 中最常見的是由長短期記憶（LSTM）單元所構成之循環神經網路。若輸入序列 （𝑥1, … , 𝑥𝑀）長度為M、輸出序列（𝑦1, … , 𝑦𝑁）長度為N。數學式為（10）式， 架構請參考圖13。 p(𝑦1, … , 𝑦𝑁|𝑥1, … , 𝑥𝑀) = ∏ 𝑝（𝑦𝑡|𝑐, 𝑦1, … , 𝑦𝑡−1） 𝑁 𝑡=1 (10) （1） 編碼（Encode） 輸入序列（𝑥1, … , 𝑥𝑀）將會經由編碼器（令函數為𝑓𝑒𝑛𝑐）編譯為表徵向 量c。若輸入序列為文字，向量c 可以理解為輸入句子的語境。 編碼器可由循環神經網路構建。循環神經網路在每一個時刻都會藉由當 前時刻的輸入𝑥𝑡與前一個時刻的隱向量ℎ𝑡−1來預測當前時刻的隱向量ℎ𝑡。隱 向量是輸入後的隱藏狀態（Hidden State），而表徵c 是所有輸入隱向量 （ℎ1, … , ℎ𝑀）經函數φ轉換後的向量，數學式可參考（11）式。 ℎ𝑡= 𝑓𝑒𝑛𝑐(𝑥𝑡, ℎ𝑡−1);    𝑐= φ({ℎ1, … , ℎ𝑀})      (11) 在循環神經網路作為編碼器的狀況下，由於循環神經網路的隱狀態ℎ𝑡本 身已包含過往狀態（ℎ1, … , ℎ𝑡−1）的資訊，所以在沒有其他機制（如注意力 機制[38]）的情況下，c 可以直接取用編碼器循環神經網路的最後一個隱向 量ℎ𝑀，即令 𝑐= ℎ𝑀。 （2） 解碼（Decode） 編碼完成之後，表徵向量c 將會進入解碼器（令函數為𝑓𝑑𝑒𝑐）中進行解 譯。當解碼器由循環神經網路構建，在每一個時刻都會藉由當前時刻的輸入 𝑦𝑡與前一個時刻的隱向量𝑠𝑡−1來預測當前時刻的隱向量𝑠𝑡。令標籤輸出序列 為（𝑦1, … , 𝑦𝑁），實際輸出序列為（𝑦̂1, … , 𝑦̂𝑁）。如（12）式。 第25 頁 𝑠𝑡= 𝑓𝑑𝑒𝑐(𝑦𝑡, 𝑠𝑡−1, 𝑐)      (12) 在模型驗證和測試階段，因為解碼器沒有標籤輸入，故使用前一個時刻 的輸出𝑦̂𝑡−1來當作當前時刻的輸入𝑦𝑡。即令𝑦𝑡= 𝑦̂𝑡−1。請參考圖14。因此 （12）式可改寫成（13）式。 𝑠𝑡= 𝑓𝑑𝑒𝑐(𝑦̂𝑡−1, 𝑠𝑡−1, 𝑐)       (13) 圖14  Seq2Seq 模型訓練和測試階段比較 圖片來源：本研究製作 圖片說明： Sequence to Sequence 模型訓練階段有正確標籤，測試階段卻無，故測試階 段以前一時刻的輸出做為當前時刻的輸入。 解碼器在形成輸出序列時，必需由向量的連續空間轉回文字的離散空間， 一個最簡單的作法是可藉由表徵向量c 與每個時刻的隱狀態𝑠𝑡運用函數或 算法g來返回對應機率最大的詞彙。如（14）式。 𝑦̂𝑡=  𝑔(𝑠𝑡, 𝑐) = max 𝑦̂𝑡  𝑝(𝑦̂𝑡|𝑠𝑡, 𝑐)        (14) 第26 頁 三、研究方法 本研究提出一種降低偏見的詞嵌入訓練方法，並將此方法動態應用到序列到 序列條件生成模型（Sequence to Sequence Conditioned Generation Framework，Seq2Seq）上做驗證。此章節將介紹研究流程、模型整體架構和技 術實作細節。
