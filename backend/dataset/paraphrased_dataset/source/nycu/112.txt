本研究採用One-Hot Encoding、tf-idf、Doc2vec 方式轉換已斷詞中文敘述欄位， 其特性及主要差異如表。 表6 預處理方法特性及主要差異 One-Hot Encoding tf-idf Doc2vec 二元特徵矩陣 鬆散矩陣 空間向量 詞的索引編碼 詞的統計資訊 預測前後文權重組合 無 單詞對所在句子及他句子 的重要性 有局部上下文資訊 模型最佳參數部分，隨機森林採用grid search 在10 到100 棵樹、葉節點最小 需要資料1 到8 間搜索，結果採用90 棵樹、Information Gain 為gini index、葉節點 最小需要資料為2。 XGboost 採用hyperopt 函式庫做參數空間搜尋，在最大深度範圍15、樹數量 範圍300、學習率0.001 到0.5、最小子權重範圍6，訓練模型的子樣本佔整個樣本 集合的比例範圍5，隨機在20 組參數、100 個測試行程中搜尋最佳參數，產出結 果最佳學習率為0.038210、最大深度4、最小子權重5，樹數量22。 MLP 則從隱藏層三層組合（10,10,10）搜尋到（100,100,100），最佳結果採用 （19,19,19）設計，共5 層。 訓練資料按上一節實驗評估結果，採用母體調整後5,210 筆資料，測試集隨機 取樣母體34％。實驗內容基本為分別對3 種預處理混合資料對3 種分類器的9 種 組合執行訓練、測試評價。
