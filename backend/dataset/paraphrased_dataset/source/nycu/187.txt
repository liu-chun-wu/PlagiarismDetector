現今泛用的循環神經網路(Recurrent Neural Network)於1980 年代就有人提出 (Elman, 1990)此架構，和前饋神經網絡(Feed-forward Neural Network, FNN) 的主要 差別在循環神經網路可以用來學習一個序列的資訊。本文中所使用的資料屬於序 列的資訊，因此選用此架構做為基礎，而在(Chung, Gulcehre, Cho, & Bengio, 2014) 與(Jozefowicz, Zaremba, & Sutskever, 2015)、(Greff, Srivastava, Koutník, Steunebrink, & Schmidhuber, 2016)等論文中可以熟悉此的架構。 循環神經網路的關鍵在於其中的隱藏狀態層(hidden layer)，可以用來儲存之前 的歷史資訊，相識於人腦的記憶，如下圖為基本RNN 架構。 圖 10  循環神經網路基本架構圖 實際的循環神經網路結構被廣泛應用於機器翻譯、語音識別、文字識別OCR 等應用。RNN 是一種有「記憶力」的神經網路，其模型也可以看成以下架構(Wang & Yeung, 2016)： 𝑋𝑡= 輸入值 ℎ𝑡= 隱藏層 𝑂𝑡= 輸出值 w = 隱藏層輸出到下一個隱藏層的權重 如上圖可以看出與前饋神經網路（Feedforward Neural Network, FFNN）最特別 的地方在於它有多了一個迴圈。若是將圖11 展開，並以方程式表示循環神經網路 的演算法，可以看到如圖12： 圖 11  循環神經網路 Y：輸入到隱藏層的權重 W：隱藏層到隱藏層的權重 V：隱藏層到輸出的權重 𝑎𝑡= ℎ𝑡−1𝑊+ 𝑋𝑡𝑌+ 𝑏𝑖                           (17) ℎ𝑡= tanh(𝑎𝑡)                               (18) 最後的輸出𝑂𝑡= ℎ𝑡V + 𝑏𝑜                                (19) 其中的𝑎𝑡是指隱藏層對輸入時的運算，其值為隱藏層ℎ𝑡−1經過權重𝑊調整後再 加上輸入值Xt 經過權重Y 調整並最後加入誤差值𝑏𝑖。其後𝑎𝑡再經過激勵函數 tanh(也可以使用Sigmoid 或Relu 等其他的激勵函數)計算後得到隱藏層的結果ℎ𝑡。 而最後的輸出值𝑂𝑡就為ℎ𝑡用權重V 調整後再加入誤差值𝑏𝑜。 由此可以看出循環神經網路在當下時間點t 的隱藏層會受到t-1 的影響，也會 繼續去影響t+1 輸出結果，這讓循環神經網路有著類似記憶的機制，此機制用在自 然語言處理時就有特別的用處，如完整的句子中每個詞彙之間都有著順序關係， 使用了循環神經網路可以讓語句輸出時更流暢且更符合人類語言。 但是循環神經網路還是有著缺點，就是輸入值的長度必須一致，且輸出值的 長度也為固定值，還有另一個缺點就是在優化模型時會根據時間順序使用反向傳 播法，如下公式(20)可以看出當資料很長t 極大時如果權重小於1，則𝑊𝑡會趨近於 0 造成隱藏層ℎ0失去作用，這就是梯度消失(Vanishing Gradient)的狀況進而會導致 模型訓練失敗，而在下一節提出新方法以解決此問題。 ℎ𝑡= 𝑊ℎ𝑡−1 + 𝑌𝑋𝑡+ 𝑏 ℎ𝑡= (𝑊2ℎ𝑡−2 + 𝑊𝑌𝑋𝑡−2 + 𝑊𝑏) + 𝑌𝑋𝑡+ 𝑏) … ℎ𝑡= (𝑊𝑡ℎ0 + ⋯) + 𝑌𝑋𝑡+ 𝑏)                   (20) 圖 12  展開的循環神經網路
