在給模型訓練集影像的答案時，由於影像經由Virtual PTZ 切割後，人物也會被切割 (如圖3-12 黃色方框所示)，無法直接使用我們一開始標注的bounding box 座標，所以我 們採用標注時的mask，並算出標注mask 中最小的(x, y)座標以及最大(x, y)座標，以此當 作訓練資料的答案。 在訓練Yolo v3 網路模型架構時，會採用Yolo v3 官方的權重加上自己的資料集去 做finetuning。由於Yolo v3 官方的權重是在COCO 資料集上訓練，裡面的Person 類別 是由一般鏡頭下的人物影像所訓練出，但因為我們研究所使用的是頂照式魚眼鏡頭下的 人物影像，人物影像有扭曲多變的情況，所以在這邊直接新增了一個類別fisheye-person， 並以這個類別作為後續的訓練對象，資料集我們採用室外A、商店A、圖書館、展場以 及展廳A～展廳C 作為訓練資料集，其餘則當測試資料集。 圖3-12 黃色方框為經virtual PTZ 轉正後被切割的人像 3.3 人物屬性實驗 本章節會依序在下面各小節說明實驗的各種方法，並將相關實驗結果呈列在第四章 節，包含辨識行人屬性的Inception v3 架構、權重的分配、以及實驗不同的loss function。
