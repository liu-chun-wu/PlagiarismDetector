In our system, we adopt RGB-D cameras in the mobile device, Lenovo phab 2 pro, to capture the information of the scene. We can get the color buffer captured by RGB camera and the 3D points captured by depth camera at respective timestamps through Google Tango API. Because the mobile device is not stationary, we cannot easily use the common foreground extraction method, such as stored the initial information of the depth image, and separate the pixels based on the distance of depth values. We supposed that the object is on a flat plane. The system first finds a proper plane model through RANSAC method, and filters the points up the plane. We project these points back to the relative position on the color image, and for each point, we take 7 Ã— 7 neighbor pixels on the color image as initial foreground. Due to the low resolution of the depth sensor, we cannot segment the plane perfectly and obtains a complete object points. Therefore, we need to spread the foreground region based on color histogram. To reduce the impact of illumination variation, here we use the Lab colour space and discard the L component. We subsample the image by 4:1 in both width and height, to give a reduced image size 1/16, and divide the image into four parts, using one thread per part for spreading to speed up processing time. After the above operation, we can get our foreground segmentation. To reduce the noise around, the flood fill algorithm is used to evaluate the connected component from center of the point set. Figure 5 shows the processing of foreground extraction. Figure 6. The process of foreground segmentation. (a) The color image from RGB camera. (b) The points filter from the segmented plane. (c) The initial foreground projects from points in (b). (d) The foreground spread result, with noise. (e) The noise reduction result of (d), which is the final result. (f) The display on mobile device
