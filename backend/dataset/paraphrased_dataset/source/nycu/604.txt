決策樹是一個預測模型；他代表的是對象屬性與對象值之間的一種映射關係。 樹中每個節點表示某個對象，而每個分叉路徑則代表某個可能的屬性值，而每個葉 節點則對應從根節點至該葉節點所經歷的路徑所表示的對象的值。決策樹僅有單一 輸出，若欲有複數輸出，可以建立獨立的決策樹以處理不同輸出。數據挖掘中決策 樹是一種經常要用到的技術，可以用於分析數據，同樣也可以用來作預測。而決策 樹主要構造如下: 1. 根結點(Root):包含所有訓練資料。 2. 結點(Node):表示一個測試屬性，依不同的分割準則所分割出來的資料。 3. 分支(Branch):節點之間的連結，一個分支代表一種分割準則。 4. 葉節點(Leaf):節點的一種，但為最末端的節點，並無節點在分支出去。 決策樹的目的在於找出具有同質性子節點分裂點，決策樹的延展會隨著節點發 展成更多同性質節點出來，而為了要將輸入的資料分類，決策樹的每一個節點即為 一個判斷式，判斷式針對一個變數去判斷輸入的資料大於或等於或小於某一個數值， 因此每一個節點可以將輸入的資料分成若干類。決策樹的優點在於可以進行的分類、 能夠同時處理連續性和類別性的資料型態，以及在進行預測和分類時明確的指出哪 一個變數重要性最大。 分類迴歸樹 (Classification Regression Tree , CART) 兼具分類與迴歸兩種功能， 是由美國統計學家Brieman 於1984 年所提出，此方法的特色是分類時一次產生兩 個節點(Node)，且應變數與自變數不限制類型，分析上較彈性，為常用的決策樹方 法。CART 會依預測變數與其相對的各項指標，將既有的訓練樣本化分成數個已知 的類別，並將其劃分程序彙總成一連串的規則(Rule)。 整體而言，分類迴歸樹 (CART)的分析流程大致可分為三個步驟，分別為:將樣本資料劃分為訓練樣本與測 試樣本，並根據劃分規則使用訓練樣本已建構最大數目的決策數；依照修剪準則， 從樹的底部向上修剪，並產生子樹群 (Subtees)，直到修剪準則被滿足為止，最後， 使用測試樣本進行交叉驗證 (Cross Validation)並在子樹群中挑選一最佳樹狀結構， 待樹狀結構建置完後，分類迴歸樹 (CART)將會產生一連串的分類規則，該規則可 幫助決策者瞭解分類規則。
