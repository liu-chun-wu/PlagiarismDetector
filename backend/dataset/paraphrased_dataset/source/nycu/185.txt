深度學習(Deep Learning)多使用在結構複雜的資料中，需要透過大量無標籤的 資料來學習，從中去獲得資料的特徵。(Schmidhuber, 2015)說明深度學習是模擬生 物的大腦神經元運作，透過輸入值與觀測值的權重以及誤差值由激勵函數 (Activation function)的調整建立模型，再使用損失函數(Loss function)對模型做出評 估，而後使用梯度下降法(Gradient decent)與反向傳播(Back Propagation)來修正參數， 降低損失函數做到模型優化，接下來各節介紹每一個階段的函數計算。 2.3.1 激勵函數 在類神經網絡中如果只將上一層的輸入做單純的線性組合(矩陣相乘)，輸出 與輸入就一定會出現線性關係，這就失去模型的意義了，所以激勵函數(Activation function)主要目就是使類神經網路成為一個非線性模型，使其可以增加複雜問題的 解釋能力，以下為常使用的類神經網路激勵函數:  S 函數(Sigmoid function)將輸入透過公式(1)轉為一個限制在0 到1 之間的非 線性輸出值。 𝑥∈R , 𝑓(𝑥) = 1+𝑒−𝑥    , 𝑓(𝑥) ∈(0,1)        (1) 圖 3  S 函數  雙曲正切函數(Hyperbolic tangent function,tanH)是雙曲函數的一種，將輸入 值透過公式(2)轉為限制在1 到-1 之間的非線性輸出值。 𝑥∈R , 𝑓(𝑥) = 𝑒𝑥−𝑒−𝑥 𝑒𝑥+𝑒−𝑥     , 𝑓(𝑥) ∈(−1,1)    (2) 圖 4  雙曲正切函數  線性整流函數(Rectified linear unit, ReLU)，是由兩段函數組成，公式(3)當輸 入小於0 時輸出為0，輸入大於等於0 時輸出等於輸入同，其優點是可以避 免當輸入接近飽和區(如sigmoid 函數的-4 與+4 之外)時會導致輸出趨近為0 而產生梯度消失，造成輸入的訊息無法透過之後小節介紹的反向傳播來傳 導。 𝑥∈R  , 𝑓(𝑥) = {0   𝑓𝑜𝑟 𝑥< 0 𝑥   𝑓𝑜𝑟 𝑥≥0    , 𝑓(𝑥) ∈[0, ∞)  (3) 圖 5  線性整流函數
