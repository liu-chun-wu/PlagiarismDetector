前小節2.1.1 所述之機器學習的「自然語言方向」之偏見研究，當前文獻 又可細分為「詞嵌入」和「聊天問答」兩種驗證觀察之模型類別。其中，「詞嵌 入」模型（詳述在2.2 節）是目前的主流切入方式。由於詞嵌入的具有的類比 關係（詳述在2.2.2 小節）經常可以映射到真實世界中概念或實體彼此之間的 相對關係，因此非常適合用來探討人類文化中的隱含偏見。 Aylin Caliska 等人（2017 年）[2]分析Glove 詞嵌入來實證心理學中「內 隱連結測驗」所揭露的人類認知偏見。內隱連結測驗（implicit association test，IAT）[18]是一種從語言量化人類的偏見的方法。它假設人類在配對兩個 他們認為相似的概念時，花費的時間較短；而配對兩個他們認為相反的概念 時，花費的時間較長。此測驗鼓勵受試者越快完成整個測驗越好，並且測量他 們對每組配對所花費的時間。研究[18]發現大部分受試者在將昆蟲標記為「不 愉快」的時間較短，標記為「愉快」則時間較長；將花朵標記為「愉快」的時 間較短，「不愉快」的時間較長。由這其中的時間差異，就可以判斷哪一組任務 對受試者來說比較簡單，而哪一組任務較簡單，則表明了受試者在潛意識中較 容易將哪一組概念聯繫起來。 內隱連結測驗透露出受試者不願明說、甚至是沒有意識到的內在深層想 法，因此常被廣泛地用來描述和解釋人類的內隱偏見，包含刻板印象的威脅 （stereotype threat）[19]。舉例來說，種族的偏見僅僅由「名字」就可以看出 來：歐洲裔美國人（通常是白人）的名字更容易被聯想為「愉快」，而非洲裔美 國人（通常是黑人）的名字更容易被聯想為「不愉快」（[18], p.1475）。又例 如，女性的名字更容易與「家庭」聯想在一起，而男性的名字更容易與「職 涯」聯想在一起（[20], p. 105）；相較於「數學」或「科學」，女性相關的詞彙 比較容易跟「藝術」和「語言」聯想在一起，而男性剛好相反[21]。 第14 頁 Aylin Caliska 等人（2017 年）[2]提出詞嵌入關聯測試（Word Embedding Association Test，WEAT）和詞嵌入真實關聯測試（Word Embedding Factual Association Test，WEFAT）兩種測試方法來評估詞嵌入所隱含的偏見程度，並 將此偏見量測方法與內隱連結測驗的相關研究結果加以比較，藉以驗證此偏見量 測方法的有效性。 Bolukbasi 等人（2016 年）[1]用he-she 等性別對去搜索Google News 訓 練之詞嵌入，利用類比關係找出與性別方向一致的職業對，例如nurse-surgeon， 為人類語料中的職業性別歧視提供間接佐證。並藉由職業中性字與性別族群字來 定義出性別方向的子空間，提出Hard de-biasing 和Soft de-biasing 兩種向量正 交方法對所有字彙去除偏見，藉以去除原本詞嵌入模型中職業的性別偏見。 在「聊天問答」模型的切入部分，Rudinger 等人（2018 年4 月）[14]設計 了句子樣板例如：「The surgeon couldn’t operate on <his/her/their> patient. It was <his/her/their> son!」並利用其中之指代（Coreference）名詞「his/her/their」 的替換填補來組成不同的句子。他們透過對句子的問卷普查，評估和確認了語意 中的性別偏見，並且和真實世界的性別職業統計資料做了正相關性的比較。 另外，Zhao 等人（2017 年）[31]的研究採用含有詞彙標籤的影像資料，在 機器學習之條件隨機場（Conditional Random Fields, CRF）模型中加入語料層 級之限制（Constraints）來做影像預測。該限制以降低詞彙出現機率之不均程度 為目標，雖然僅在字彙範圍操作、沒有涉及句子，但沒有使用到詞嵌入，僅使用 計數值。此研究巧妙地連接了「自然語言」與「影像識別」任務，在不影響預測 正確率的情形下進行影像識別任務的去偏見化。 本研究基於Bolukbasi 等人（2016 年）[1]基於詞嵌入去偏見化的研究加以 改良。本研究使用了序列到序列條件生成模型（Seq2Seq）（詳述在2.3 節）訓 練之聊天機器人為主要模型架構，加入一以詞嵌入中立性為目標之限制 第15 頁 （Constraints），來達成深度學習序列模型端對端的去偏見化。本研究亦從人類 社會之族群歧視觀點作為啟發，試圖將中性字的定義和適用範疇加以擴展，以驗 證模型的泛化。
