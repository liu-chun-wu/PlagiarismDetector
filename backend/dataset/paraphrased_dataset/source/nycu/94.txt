向量空間模型VSM （Vector Space Model）是把每個文件看做為一整串的索 引（Index Term）所組成，而每一個詞都有權重，不同的索引詞根據自己在文件中 的權重進而影響文件相關性的評分計算，查詢及文件都可以轉化為索引詞及權重 組成[12]。 許多機器學習算法需要輸入表示為固定長度的特徵向量。當涉及到文本時， 其中最多的一個常見的固定長度功能是詞袋。儘管它們很受歡迎，但仍然具有詞 彙特徵有兩個主要的缺點：它們失去了單詞的順序，也忽略了語義[9]。 在空間向量模型的如果只是換一個形式記錄文本索引的話，佚失文本的前後 關係的缺點並沒有改善，所以在Tomas Mikolov 的多篇論文中，希望透過訓練時把 前後文考慮進去來訓練模型。 Word2vec 為現在詞嵌入（word embededing）相關領域主流產生詞向量的技術， 為Tomas Mikolo 在Google 帶領的研究團隊創造，採用淺層雙層的神經網路，並訓 練以重新建構詞文本。網路以詞表現，猜測相鄰位置的輸入詞。 詞袋模型假設下，詞的順序並非重點，而Word2vec 模型訓練方法則以預測上 下詞彙為核心，分為 skip-gram 模型和 CBOW 模型（圖），skip-gram 模型使用 目標詞彙來預測它的上下文詞彙，而 CBOW 模型使用上下文詞彙來預測目標。 其中，訓練過程中產生的隱藏層的權重組合可以當作這個方法試圖尋找的「詞 向量」，並且加以應用。 舉例來說，「空間向量模型以前後文關係為訓練標的」這句話，訓練方式就 是用「向量」預測「空間」、「模型」，中間的權重值組就是詞向量，所以衍生 的結果就是在文法上相似位置的詞彙會有極高的相似度[10]。 圖2 skip-gram 模型和 CBOW 模型比較[10]
