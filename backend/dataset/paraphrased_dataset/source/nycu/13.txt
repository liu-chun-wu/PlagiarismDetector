Yolo 為2018 年提出的物體偵測神經網路架構，當時在[17][40]兩個資料集皆為 state-of-the-art，同時能達到實時(real-time)偵測。本篇為yolo 系列的第三代，方法與前 一代相似，主要以兩種方式改良網路架構，首先，它使用了resnet[32]中的殘差網路架 構設計更深的網路，其次，它參考了FPN 的架構設計，加入多尺度預測以及多尺度特 徵融合。 在物體偵測的方法上，yolo 採用one-stage 的方式，直接從圖片中偵測定界框 (bounding box)，其方式如下，首先將圖片等分成S × S個區塊(如圖13 所示)，而每個區 塊會有N 個事先設計好的anchor box，而神經網路則負責預測每個anchor box 的長寬 變化、中心位置位移以及所屬類別。網路估計結果與定界框關係可參考Eq. (4)到Eq. (7)， 𝑏𝑥= 𝜎(𝑡𝑥) + 𝑐𝑥 (4) 𝑏𝑦= 𝜎(𝑡𝑦) + 𝑐𝑦 (5) 𝑏𝑤= 𝑝𝑤𝑒𝑡𝑤 (6) 𝑏ℎ= 𝑝ℎ𝑒𝑡ℎ (7) 其中𝑡𝑥, 𝑡𝑦, 𝑡𝑤, 𝑡ℎ為網路估計的結果，(𝑏𝑥, 𝑏𝑦)為定界框中心位置，(𝑏𝑤, 𝑏ℎ)為定界框 的寬與長，(𝑝ℎ, 𝑝𝑤)為anchor box 的寬與長，(𝑐𝑥, 𝑦𝑦)為anchor box 所在區域左上角的位 置，𝜎()則代表羅吉斯函數。 圖13、圖片等分𝐒× 𝐒個區塊示意圖，圖片出自[42] 第四章、問題描述及系統架構
