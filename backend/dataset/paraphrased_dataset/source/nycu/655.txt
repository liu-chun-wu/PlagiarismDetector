有鑑於近年來人工智慧（Artificial Intelligence）與機器學習（Machine Learning）正以鋪天蓋地之姿被廣泛運用到日常生活中，許多策略開始仰賴機器 自動化地執行。因此，機器執行策略的中立性、公平性和倫理道德問題也越來越 受到重視。 一般人容易直覺地假設，機器所學到的知識應該是中性的、公平的――因為 它來自數學的計算和推導。然而事實並非如此。由於機器透過來自人類日常生活 的數據，例如日常對話、影像、行動數據等來學習，因而能夠捕捉到人類的生活、 第3 頁 甚至是知識系統與文化。因此，數據驅動（Data-driven）的機器學習演算法亦不 可避免地捕抓到了人類的偏見和刻板印象。 在機器學習中，「偏頗」（Bias）是必然的。「偏頗」是指先驗資訊 （Prior），是過去資訊累積之經驗所提供的機率分布，也是一個智能體必要的先 備知識[6]。換言之，一個毫無偏頗的機器學習模型，同時也是一個沒有學習到 任何知識的模型，其預測結果是不可信的。 對智能體而言「偏頗」雖是必要的，卻也可能因為資料來源的不公正，而形 成「有害的偏頗」，本研究暫稱之為「偏見」。由於數據僅是呈現相關性事實，並 不涉及道德人權，也無法說明事件的因果，「有偏見的演算法」往往在應該給予 公平機會的場域（例如職場、執法、操作工具等），僅因「膚色」和「性別」等因 素有較差的預測率或是產生有差異的預測結果。 早在2002 年，語音辨識的性別偏見問題已經被提出。語音辨識系統對於女 性的聲音往往辨識率比男性低[8]，而醫療用聲控指示系統在男性操作時表現得明 顯更好[9]。又例如，Google 廣告更常將高薪的工作（例如：CEO）鎖定男性而 非女性投放[10][11]。 犯罪預測的演算法經常鎖定黑人多於白人[7]，這可能會助長種族歧視的執法 行為。另一個著名的例子是臉部辨識中所隱含的種族歧視。2015 年，Google 的 相片應用程式因為將黑人的臉辨識為大猩猩，甚至完全忽視黑人的臉，只辨識出 周遭的物品和情境，因而受到指責（參考圖2）。2018 年[12]有研究指出，無論 是在Microsoft、IBM 或是Face++的臉部辨識系統中，深膚色人種和女性比起淺 膚色人種和男性的臉部辨識率都要來得低。 第4 頁 圖2  Google 相片的種族歧視爭議﹙2015 年﹚ 圖片來源: 推特帳號jackyalcine 的發表文章1 圖片說明: Google 的相片應用程式對深膚色人種的臉部辨識率不佳，2015 年
