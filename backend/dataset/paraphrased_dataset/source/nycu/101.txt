在十分早期的中文特徵擷取分類器實驗，針對tf-idf 編碼方式，KNN 及SVM 時 常被使用，但本研究除了中文欄位外還有其他數值欄位，考量這一點，在傳統方 式上採用隨機森林，對於混和資料也有相對平均的表現[14]。 隨著Kaggle 競賽的蓬勃，混合欄位的資料分析中，XGboost（eXtreme Gradient Boosting）在訓練時間及精確度受前幾名獲獎隊伍的青睞，尤其不受限固定長度欄 原始資料 中文資料預處理 其他欄位（數值資料）預 處理 處理後資料合併 訓練資料 測試資料 訓練分類模型 Doc2vec tf-idf One-Hot Encodin MLP XGBoost 隨機森林 預測分類/評價 位的限制，整體評價上又優於隨機森林，是分類樹模型的優勢選項，可以對隨機 森林對比，也可以對類神經網路方法比較。 另外本研究希望能測試採用類神經網路的效益，這也是研究重點之一—預處理 與類神經網路的適性，這將是除了模型基礎結構或消耗資源外需要被考量的重大 因素，尤其是機關有打算導入機器學習—類神經網路組合模型，優化或精度提升的 重要議題。因此，本研究同時也將MLP（Multilayer Perceptron）作為這次研究方 法之一，用來比較預處理方法對類神經網路的適性。
