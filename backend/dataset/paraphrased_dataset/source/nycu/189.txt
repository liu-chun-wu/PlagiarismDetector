前章節中提到循環神經網路會因為資料過長而導致訓練越後面前面資訊會被 遺忘的情況，而這節介紹的長短期記憶（Long Short-Term Memory）(Hochreiter & Schmidhuber, 1997)就是為了解決這個問題而設計的隱藏層架構。如下圖所示，將 長短期記憶用在循環神經網路中用來實現隱藏層內部的處理邏輯： 圖 14  長短期記憶隱藏層內部邏輯 註:引自(Olah, 2015) 長短期記憶中有 3 個閘（Gates）來控制隱藏層在不同時間點的記憶狀態，來 決定各項資訊是否要繼續儲存或是遺忘： Forget Gate：遺忘閘決定細胞是否要遺忘目前的記憶狀態。 Input Gate：輸入閘決定目前輸入有是否值得處理。 Output Gate：輸出閘決定更新後的記憶狀態有多少要輸出。 並且這些閘的權重也是經由輸入資料訓練出來的，透過這些閘控管機制，長 短期記憶可以將舊有的記憶狀態保存下來，並且在需要的時候再拿出來使用，以 下將每個閘的動作詳細說明。 圖 15  遺忘閘路徑 註:引自(Olah, 2015) 𝑓𝑡= 𝜎(𝑊𝑓∙[ℎ𝑡−1, 𝑋𝑡] + 𝑏𝑓)                        (24) 𝑓𝑡: 遺忘閘的函數。 𝜎: 激活函數（S 函數）。 𝑊𝑓: 遺忘閘的權重。 ℎ𝑡−1: 在t-1 時的隱藏層。 𝑋𝑡:在t 時的輸入值。 𝑏𝑓:遺忘閘的誤差值。 在長短期記憶神經網路層中遺忘閘的目的在於確認上一階段的隱藏層狀態是 否需要保留(Olah, 2015)，輸入層的𝑋𝑡會與前一時間點(t-1)的隱藏層狀態ℎ𝑡−1做堆疊 合併得到[ℎ𝑡−1, 𝑋𝑡]，經權重𝑊𝑓調整後加上誤差值𝑏𝑓，最後再透過激勵函數𝜎的判斷 做為遺忘閘(Forget gate)的輸出。 圖 16  輸入閘路徑與候選神經元狀態路徑 註:引自(Olah, 2015) 𝑖𝑡= 𝜎(𝑊𝑖∙[ℎ𝑡−1, 𝑋𝑡] + 𝑏𝑖)                  (25) 𝐶̃𝑡= 𝑡𝑎𝑛ℎ(𝑊𝑐∙[ℎ𝑡−1, 𝑋𝑡] + 𝑏𝑐)                 (26) 𝑖𝑡: 輸入閘的函數。 𝜎:激勵函數。 𝑊𝑖: 輸入閘的權重。 ℎ𝑡−1:在t-1 時的隱藏層。 𝑋𝑡:在t 時的輸入值。 𝑏𝑖:輸入閘的誤差值。 𝐶̃𝑡: 候選神經元狀態。 𝑡𝑎𝑛ℎ():激活函數（hyperbolic tangent）。 𝑊𝑐: 候選神經元狀態的權重。 𝑏𝑐: 候選神經元狀態的誤差值。 輸入閘用來判斷目前輸入值是否需要進入隱藏層:由公式(25)中可以知道 ℎ𝑡−1, 𝑋𝑡 經過權重𝑊𝑖調整後加上誤差值𝑏𝑖，再經由激勵函數𝜎的判斷做為輸入閘 (Input gate)的輸出。再來由公式(26) t 時的候選神經元狀態(cell state)透過ℎ𝑡−1, 𝑋𝑡 經權重𝑊𝑐調整後加上誤差值𝑏𝑐，再經由激勵函數𝑡𝑎𝑛ℎ()的判斷，來決定是否需要將 值傳遞到t+1的隱藏層神經元。 圖 17  Cell state 路徑 註:引自(Olah, 2015) 𝐶𝑡= 𝑓𝑡⊙𝐶𝑡−1 + 𝑖𝑡⊙𝐶̃𝑡                     (27) 𝐶𝑡: 在t 時的神經元狀態值。 𝑓𝑡: 在t 時的遺忘閘輸出值。 𝐶𝑡−1: 在t-1 時的神經元狀態值。 𝑖𝑡: 在t 時的輸入閘函數值。 𝐶̃𝑡: 在t 時的候選神經元狀態值。 由公式(27)所示當前神經元狀態𝐶𝑡是將遺忘閘的輸出𝑓𝑡與t-1 的神經元狀態𝐶𝑡−1做 乘積再加上輸入閘的輸出𝑖𝑡與當前候選神經元狀態𝐶̃𝑡做乘積，且𝐶𝑡會被下一時間點 t+1 中在被使用。 圖 18  Output gate 與hidden-layer 在時間t 時路徑 註:引自(Olah, 2015) 𝑂𝑡= 𝜎(𝑊𝑜∙[ℎ𝑡−1, 𝑋𝑡] + 𝑏𝑜)                      (28) ℎ𝑡= 𝑂𝑡⊙𝑡𝑎𝑛ℎ (𝐶𝑡)                            (29) 𝑂𝑡: 在t 時的輸出閘函數。 𝜎: 激勵函數(Sigmoid function) 。 𝑊𝑜: 輸出閘的權重。 ℎ𝑡−1:在t-1 時的隱藏層。 𝑋𝑡:在t 時的輸入值。 𝑏𝑜:輸出閘的誤差。 ℎ𝑡:在t 時的隱藏層。 𝑡𝑎𝑛ℎ():激勵函數(hyperbolic tangent) 。 𝐶𝑡: 在t 時的神經元狀態值。 由公式(28)可以看出輸出閘𝑂𝑡透過[ℎ𝑡−1, 𝑋𝑡]與權重𝑊𝑜調控且加上誤差值𝑏𝑜，後在由 激勵函數σ 判斷做為輸出閘輸出。再由公式(29)可看出當前隱藏層ℎ𝑡是由輸出閘𝑂𝑡 與經過激勵函數判斷的神經元狀態𝐶𝑡做乘積。
