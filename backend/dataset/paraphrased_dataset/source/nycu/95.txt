引用自Tomas Mikolov 最初的想法，空間向量表述文本，應該要包含詞序或語 意，否則同一句話如果包含相同的詞組，將會被表示為相同的向量。Word2vec 的 訓練過程中就是有考慮到了詞的順序性，所以才能將相似的詞聚在一起，所以引 用了Word2vec 的訓練方法，對文本衍生出了Doc2vec 的處理方針[9]。 圖3 PV-DM 模型[9] Doc2vec 主要也分為兩種模型，分別為CBOW 所衍生出的PV-DM（Paragraph Vector: A distributed memory model）（圖）及Skip-gram 所衍生出的DBOW （Distributed bag of words）（圖），在Word2vec 模型基礎上加入文章id 的輸入， 隱藏層的權重依然是標記向量的來源，透過訓練詞序關係時得到文章向量矩陣， 定義空間中每個文章的位置。 圖4  DBOW 模型[9]
