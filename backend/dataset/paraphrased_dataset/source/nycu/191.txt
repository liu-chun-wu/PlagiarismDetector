在序列到序列結構中會將輸入的所有序列都編碼成統一的語意向量Context， 所以其中包含原序列的所有訊息，但是當輸入序列過長時會導致Context 無法存下 全部的特徵，而造成decode 時的效果下降。此時可以使用注意力機制來加強decode 時的輸出層對輸入的向量，以下介紹所使用的基本注意力機制(Luong Attention)(Luong, Pham, & Manning, 2015) ，而在(Weng et al., 2018)也使用此機制 來強化序列到序列機來做為英文會話語音識別。 圖 21  注意力機制 圖21 中ℎ̅𝑆與ℎ𝑡 (都為[𝑛ℎ, 1]的向量)分別表示Encoder 狀態與Decoder 狀態，ℎ̃𝑡 表示注意力機制層輸出的最終Decoder 狀態。來計算decoder 在t 時刻隱藏層狀態ℎ𝑡 對Encoder 每一個隱藏層狀態的ℎ̅𝑆權重𝑎𝑡(𝑠)數值: 𝑎𝑡(𝑠) = exp (𝑠𝑐𝑜𝑟𝑒(ℎ𝑡,ℎ̅𝑠)) ∑ exp (𝑠𝑐𝑜𝑟𝑒(ℎ𝑡,ℎ̅𝑠′)) 𝑠′ (30) 公式中的score 可以用以下三種方式計算 score(ℎ𝑡, ℎ̅𝑠) = { ℎ𝑡 𝑇ℎ̅𝑠                                                        𝐷𝑜𝑡 ℎ𝑡 𝑇𝑊𝑎ℎ̅𝑠                                          𝐺𝑒𝑛𝑒𝑟𝑎𝑙 𝑣𝑎 𝑇tanh (𝑊𝑎𝑐𝑜𝑛𝑐𝑎𝑡(ℎ𝑡, ℎ̅𝑠))      𝐶𝑜𝑛𝑐𝑎𝑡 其中的Dot 是使用向量內積，General 則是通過乘以𝑤𝑎(是[𝑛ℎ, 𝑛ℎ]的矩陣)權重再進 行計算，Concat 是用激勵函數下去計算。 利用權重𝑎𝑡(𝑠)就可以計算隱藏層加權後的𝑐𝑡(為[𝑛ℎ, 1]向量)的Context 狀態向 量。 𝑐𝑡= ∑𝑎𝑡(𝑠)ℎ̅𝑠 𝑠 (31) 再與原來的Decoder 層隱藏層序列狀態ℎ𝑡合併起來，再算激勵函數取得 ℎ̃𝑡。 ℎ̃𝑡= tanh (𝑊𝑐∙𝑐𝑜𝑛𝑐𝑎𝑡(𝑐𝑡, ℎ𝑡)) (32) 𝑐𝑡與ℎ𝑡大小都為[𝑛ℎ, 1]的向量，所以這邊使用全連接𝑊𝑐，以防止Decoder 的cell 變 大。 最後再將已經過注意力機制後的ℎ̃𝑡向量乘以最後的輸出權重𝑊ℎ𝑜矩陣取的t 時間的 輸出𝑦𝑡。 𝑦𝑡= 𝑊ℎ𝑜ℎ̃𝑡+ 𝑏ℎ𝑜 (33)
