支持向量機﹝SVM﹞是實現結構風險最小化的學習機器歸納原則，以獲得有 限數量學習模式的良好化。結構性風險最小化﹝SRM﹞涉及同時嘗試最小化經驗 風險和VC﹝Vapnik-Chervonenkis）維度。這個理論最初是由Vapnik 和他的同 事基於AT&T 貝爾實驗室的分離雙分區問題。SVM 實現了一個學習算法，用於識 別複雜數據集中的細微模式很有用。該算法執行區分性分類通過示例學習以預 測先前未看到的數據的分類[11]。 在Support Vector Machine 裡面有兩種方式：分類：Support Vector Classification ﹝SVC﹞、回歸：Support Vector Regression﹝SVR﹞。傳統 統計/回歸過程通常著重於於為導出函數f(x)的過程中所有訓練樣例的預測和 實驗觀測響應之間的偏差最小化。支持向量回歸﹝SVR﹞的主要特點之一是不在 於觀察到的訓練錯誤時將誤差最小化，SVR 嘗試最小化廣義誤差界限以實現廣 義性。這個概化誤差界限是結合訓練誤差和正規化項於控制假設空間的複雜 性。用於回歸的SVM 的一個版本在1997 年由Vapnik，Steven Golowich 和 Alex Smola 提出，這種方法被稱為支持向量回歸﹝SVR﹞[5]。 SVR 是在SVMs 中被運用最廣泛的形式，是一種support vector﹝SV﹞機器 在於迴歸和預估功能時基礎理論的概括[4]。它包括了目前用於訓練支持向量機 的演算法彙總，包括編程部分和用於處理大型數據集的進階方法。 假設訓練資料﹝training data﹞表示為(𝑥1,𝑦1),..., (𝑥1,𝑦1) ∈R,d×R,其 中x 表示輸入的「特徵﹝attributes﹞」，y 表示該特徵所對應的迴歸值﹝相 當於SVM 中的目標類別， target class﹞。令f(𝑥) = 𝑤∙𝑥+ 𝑏，w ∈R, b ∈ R，如果對每個instance xi 而言，f(𝑥𝑖)和𝑦𝑖的差值都小，則我們知道這樣的 f(𝑥)能從𝑥準確地預測y，這個w 即是SVR 所要找的平面。 𝑚𝑖𝑛𝑖𝑚𝑖𝑧𝑒1 2 ‖𝒲‖2 subject to‖𝑦𝑖− (𝒲∙𝑥𝑖−𝑏)‖ ≤ε   [4] 其中ε≥0，用來表示SVR 預測值與實際值最大的差距，而此演算法也因此而得 名，稱為ε-SVR。 在ε 合理的情形下﹝例如給定一個大得過份的ε 就是不合理﹞，如果從 式1 能出解，這種情形稱為feasible。然而大多數的應用中，因為有雜訊、誤 差等等各種因素，通常不會是feasible 的情形，因此我們要加入額外的項,以 容許某些instances 落在ε 之外。 [4] 在式2 中，每個training instance 都有其對應的ε 及ε𝑖 ∗，用來決定該 training instance 是否可以落在ε 的範圍之外。而C 的作用則如同在SVM 裡 一般，用來調整訓練模型﹝training model﹞是否過份或不足調適資料 ﹝overfitting 或nderfitting﹞。 圖 7 SVR 出處： A tutorial on support vector regression 而本研究因數據集是迴歸分布，所以採用SVR 進行實驗分析。
