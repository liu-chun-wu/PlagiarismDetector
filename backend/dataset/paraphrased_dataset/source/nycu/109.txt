Ｗord2vec 最理想的使用方式是直接運用訓練好的模型，才具有足夠的廣度且 不需要重新訓練，以英文為例，Tomas Mikolo 在Google 帶領的研究團隊有提供訓 練好高度泛化的模型，但中文的狀況一般來說訓練好的模型幾乎沒有免費提供， 所以運用Ｗord2vec，需要先利用所有的文本進行重新訓練，這一個動作實際上與 任何一種空間向量模型的重新編碼無異，這在所有空間向量解決中文轉換是共通 的問題。Ｗord2vec 本身的優勢在於訓練方式因是利用預測前後文作為訓練基準， 所以「不需要事先標記句子的特徵」及「可以囊括局部前後文資訊」。 如果在於重新訓練為必要的前提下，從Ｗord2vec 方法所衍生的Doc2vec 便是 不錯的方針，同樣出自於Tomas Mikolo 團隊的想法，差別在於將文本id 加入模型 訓練，訓練好的模型相當於一個空間向量型的句子索引，特徵表現上能比較出兩 個句子在前後文上的相似程度，對分類工作有更多的可用因子。
