在循環神經網路中已可以從過去的上下文來取得輸出，那雙向循環神經網絡 （Bidirectional Recurrent Neural Network）(Schuster & Paliwal, 1997)就是更增加了來 自未來的上下文來強化輸出的準確度，在中文中時常省略連接詞(因為、所以)，就 可以透過此架構來強化後面語句對前面語句的關係。 圖 13  雙向RNN 架構圖 架構中增加了一層隱藏層ℎ′𝑡以及權重W′、Y′、V′，而在各層的輸出公式如下: 𝑎′𝑡= ℎ′𝑡+1𝑊′ + 𝑋𝑡𝑌′ + 𝑏𝑖 (21) ℎ′𝑡= tanh(𝑎′𝑡) (22) 𝑂𝑡= ℎ𝑡𝑉+ ℎ′𝑡𝑉′ + 𝑏𝑜 (23) 其中的𝑎′𝑡是指隱藏層對輸入時的運算，其值為隱藏層ℎ′𝑡+1經過權重𝑊′調整後 再加上輸入值𝑋𝑡經過權重𝑌′調整並最後加入誤差值𝑏𝑖。其後𝑎′𝑡再經過激勵函數 tanh(也可以使用Sigmoid 或Relu 等其他的激勵函數)計算後得到隱藏層的結果ℎ′𝑡。 而最後的輸出值𝑂𝑡為ℎ𝑡用權重V 調整後加上ℎ′𝑡用權重𝑉′調整再加上誤差值𝑏𝑜。
