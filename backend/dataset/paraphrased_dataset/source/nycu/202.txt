本研究是使用序列到序列架構建立的抽象式摘要模型，有別於傳統擷取文本 的方式，可以透過編碼將文本語意收集後在經過解碼來取得相同語意的簡短摘要， 而在建構序列到序列架構時選用雙向循環神經網路增加了未來時間的語意資訊輸 入進隱藏層的方式相較一般單向循環神經網路有較好的摘要能力。而在訓練模型 的參數選用會因為底層架構的差異而有所變化，本次研究訓練模型的最好成績是 在雙向循環神經網路層數最多(512 層)及詞向量維度最多(500 維)的組合下訓練出 來的。而在摘要類似新聞格式的中文文本時,選字搜索方式在使用貪婪搜索與集束 寬度為2 的集束搜索會有較好的抽象摘要能力。 Model ROUGE-1 ROUGE-2 ROUGE-L RNN+Context+Char 0.299 0.174 0.272 本研究 0.317789261 0.188376717 0.26418374 表格 12 與前研究的成績比較 本研究最佳的成績與前研究(Hu et al., 2015)的比較如表12，用3.5 節的ROUGE 評估工具來比較(其各指標最佳值為1)，由ROUGE-1 來看出現字詞的涵蓋率，本 研究有較高的平均成績，而ROUGE-2 來看字詞的流暢度本研究也有較高的平均成 績，由前兩個指標來看本研究是更符合人類語法的摘要模型。 文本 10 月28 日兩市已有91 家房企披露三季報公司前三季度存貨合計達到 915553 億元同比增長2408 今年開發商以價換量銷售策略顯效三季度房企 存貨周轉率較大改善樓市庫存速度明顯提升（中國證券報） 參考摘要 91 家房企前三季度存貨近萬億元 自動摘要 91 家房企前三季度存貨近千億元 表格 13 最高分的單一文本與摘要 而用整體來看，其中最好的單一文本自動摘要涵蓋率有到0.93 分如表13，雖 然涵蓋率很高，但是遇到數字的解讀時’萬’與’千’的差異造成錯誤的訊息。此問題 需要再將模型對數字處理的能力再改善才能有更好的摘要能力。 文本 高官相繼落馬牽扯眾多上市公司高管精心搭建政商朋友圈逐漸浮出水面 合作背後利益均沾2014 年中紀委強勢開展打虎行動周永康令計畫一干高 官相繼落馬 參考摘要 揭秘“老虎們”的上市公司朋友圈合作背後利益均沾 自動摘要 周永康令計畫一干高官落馬 表格 14 各指標為0 的單一文本與摘要(由簡體轉為繁體) 而在各指標都為0 的單一文本中，如果單看自動摘要也十分符合這篇文本的 主要內容，但是與參考指標來進行ROUGE 比較時各指標分數就會為0，所以要如 何避免類似情況也是將來的課題。
