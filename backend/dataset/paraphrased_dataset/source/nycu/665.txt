分佈式表示（Distributed representation）的概念於1986 年由Hinton [22] 首次提出。相較於獨熱表示是高維度的整數向量，分佈式表示則通常是較低維度 的實數向量，且分佈式表示通常是由高維度的獨熱表示經前饋類神經網路（Feed- Forward Neural Network）壓縮到較低維度而成。 圖9  分佈式表示（Distributed representation）詞向量的訓練模型架構 第17 頁 圖片來源：[23] 圖片說明：Bengio 等人於2003 年提出之分佈式表示詞向量的訓練模型架構，藉由序列中 的單詞的前後字彙來訓練該單詞的分佈式表示。分佈式表示可以理解為一種濃縮後的獨熱表示， 可以去除冗餘維度，使維度利用最大化。 為進一步解決0/1 表示在自然語言處理所面臨的問題，2003 年Bengio 等人 [23]語料庫中的單詞序列以N-gram 模型和類神經網路做訓練，得到各個單詞的 分佈式表示，模型架構可參考圖9。 Bengio 研究[23]中指出，分佈式表示可以減少維度詛咒，以提高語言模型的 泛化（Generalization）能力。機器學習中，泛化是指訓練模型對於從未看過之數 據集的適用能力，而提升泛化能力往往是整個機器學習的目的。在語言模型中， 提升泛化能力是指可以從通過已經出現句子的單詞組成，對於預測從未出現的單 詞序列可以獲得較高的機率分佈。 當前所稱的詞嵌入（Word Embedding）是指這種單詞的分佈式表示法。它 通常由語料中之句子經由類神經網路訓練而成，詞彙之間的關係經常可以由計算 詞與詞之間向量表示的餘弦相似度（Cosine similarity）或歐式距離（Euclidean distance）來得到，因此詞嵌入常可理解為單詞之抽象概念在數學空間中的表示， 或可稱之為詞表示（Word representation）。 詞嵌入訓練的模型和方法有很多。有鑑於使用傳統類神經網路模型的訓練時 間過長，2013 年 Tomas Mikolov 等人提出的兩篇論文（[24][25]）提出連續型連 詞袋模型（Continuous Bag-of-Words Model，CBOW）與跳躍式模型（Skip-Gram Model，SG），使用階層軟式最大化（Hierarchical Soft-max，HS）[24]以及負例 採樣（Negative Sampling，NS）[26]方法提供一個稱作word2vec 的工具包，裡 面包含數種詞嵌入的方法。其中之一的特點是訓練速度快，另一個特點在於得到 的詞表示具備類比（Analogy）性質。類比性質類似於「A 之於B，相當於C 之 第18 頁 於D」這樣的結構，舉例來說，「巴黎之於法國，相當於羅馬之於義大利」這樣 首都對國家的類比關係，以及「small 之於smaller，相當於large 之於larger」 這樣的文法關係。請參考圖10。 圖10  詞嵌入的類比性質﹙GloVe 訓練法，2014 年﹚ 圖片來源：論文[27]、Standford 大學之網站連結2 2 https://nlp.stanford.edu/projects/glove/ 第19 頁 圖片說明：分佈式表示的詞向量，在空間中的距離和方向能夠大致符合其單詞之間的語意 關係。例如圖10﹙上﹚顯示出一種文法關係的類比性，而圖10﹙下﹚顯示出一種性別關係的類 比性。 類比關係表現在詞向量的數學運算，大致會滿足「A-B=C-D」這樣的關係。 以前述之例子來看，表現出來的數學關係大致如下： 𝑃𝑎𝑟𝑖𝑠 ⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗ −𝐹𝑟𝑎𝑛𝑐𝑒 ⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗ ≅𝑅𝑜𝑚𝑒 ⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗ −𝐼𝑡𝑎𝑙𝑦 ⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗ 𝑠𝑚𝑎𝑙𝑙 ⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗ −𝑠𝑚𝑎𝑙𝑙𝑒𝑟 ⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗ ≅𝑙𝑎𝑟𝑔𝑒 ⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗ −𝑙𝑎𝑟𝑔𝑒𝑟 ⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗ Tomas Mikolov 認為[25]，具備類比關係的詞嵌入能夠捕捉到整體詞彙的語 意結構，表示該詞嵌入性質較佳。故類比關係的強弱有無，也常用來衡量一詞嵌 入的訓練好壞。 後續有眾多研究針對詞嵌入的訓練做改進，例如：使用了全局信息的 GloVe[27]、以完成詞性標註（Part-of-Speech tagging，POS tagging）、短語識 別（CHUNK）、命名實體識別（Named Entities Recognition，NER）和語義角 色標註（Semantic Role Labeling，SRL）等任務為目的的預訓練詞嵌入方法[28] 等。
