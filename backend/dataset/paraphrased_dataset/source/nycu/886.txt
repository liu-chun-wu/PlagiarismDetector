Due to the poor efficiency of ICP on the mobile device, we also adopt the device motion tracking to improve the efficiency. Since the pose module of google tango API can get the pose of the device at specified timestamp, we use the API to get the poses of the depth sensor in different timestamp and computing the transform matrix of the device. The point cloud ğ‘€ğ‘‘ğ‘Ÿğ‘ğ‘¤ of tracking result that we draw on the screen, the OpenGL overlay, can be described as below. ğ‘€ğ‘‘ğ‘Ÿğ‘ğ‘¤= ğğ•ğŒğ‘€ğ‘šğ‘œğ‘‘ğ‘’ğ‘™,                     (28) ğŒ= ğ‘ƒğ‘‘ğ“,                           (29) where ğ  is the projection matrix, ğ•  is the view matrix, ğŒ  is the model matrix, and the multiplication of these three matrix is the MVP matrix in OpenGL. The model matrix is composed of the transform matrix from depth camera space to OpenGL space ğ‘ƒğ‘‘ , which defined as depth pose, and the model pose of tracking result ğ“. ğ‘€ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ is the origin point cloud of the input model in depth camera space. The matrix ğ and ğ• can also available from the API. We suppose that the point cloud we draw in next frame as ğ‘€ğ‘‘ğ‘Ÿğ‘ğ‘¤ â€² , and ğ‘€ğ‘‘ğ‘Ÿğ‘ğ‘¤ â€² = ğâ€²ğ•â€²ğ‘ƒğ‘‘ğ“ğ‘€ğ‘šğ‘œğ‘‘ğ‘’ğ‘™. The new transform matrix and the new point cloud of the model can derive from the depth pose ğ‘ƒğ‘‘ â€² in next frame. The new model pose ğ“â€² is described as ğ“â€² = ğ‘ƒğ‘‘ â€² âˆ’1ğ‘ƒğ‘‘ğ“.                     (30) We can continue using the new pose in ICP tracking or pose smoothing in section 5.4. We also do the pose measurement every ğ‘ frames, to check whether the pose drifts too much or not. If ğ¸ğ‘¡ğ‘Ÿğ‘ is larger than ğ‘‡ğ·ğ‘€, the device motion tracking is marked failure, and will back to ICP tracking state.
