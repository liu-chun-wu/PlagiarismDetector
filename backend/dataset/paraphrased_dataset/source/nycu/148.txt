最早的CNN 是在1989 年由LeCun 所發明，並命名為LeNet[4]，然而，由於硬體的 限制及演算法設計尚未完備，故CNN 的效率並未被重視，其成效亦未優於SVM 等特徵 提取方式。而隨著時間的演進，硬體及技術的進步，開始出現了ReLU Function、丟棄層 (Dropout Layer)、數據增加(Data Augmentation)、GPU 的平行運算方式以及大數據(Big Data)，因此在2012 年時出現了突破性的CNN 架構AlexNet[5]。 AlexNet 在2012 年時，獲得了ImageNet 圖像分類競賽中冠軍，且錯誤率與去年的 冠軍相比大幅提升，這也讓CNN 這種深度神經網路再次被世人所重視，而AlexNet 最 主要的成功原因則是前述所提到的ReLU Function、丟棄層(Dropout Layer)、GPU 的平行 運算方式以及大數據(Big Data)的出現。ReLU Function 使得神經網路的收斂速度加快； Dropout Layer 能透過適當的丟棄不重要的神經元使神經網路的誤差減少，且避免模型過 度擬合(Over Fitting)的產生；數據增加(Data Augmentation)則是透過增加數據的方式，來 增加訓練次數藉此提升神經網路分類準確率，而最重要的無非是GPU 的出現，GPU 透 過平行計算方式，大大的減少了神經網路的訓練時間。 在2012 年AlexNet 的出現後，陸續有不同的學者提出不同的CNN 架構。2013 年網 路中的網路(Network In Network, NIN)[9]的想法被提出，其特點在於使用11 的Filter 進 行卷積計算，為卷積層帶來更多的組合性，使用少量的參數在特徵的所有像素中共享， 這個想法之後也被應用在不同的神經網路架構中，例如：ResNet 及Inception。 接著，在2014 年所出現的VGG 神經網路是由牛津大學計算機視覺組(Visual Geometry Group)和Google DeepMind 公司的研究員一起研發的[10]，其特點在於使用多 次的33 Filter 進行卷積計算，重複使用以提取出更複雜的特徵，但也因為這樣導致VGG 的訓練速度緩慢，且權重值數量過多。 同年2014 年，GoogLeNet[11]的出現打敗了原先的VGG 神經網路，其特別之處在 於其中的Inception 微架構，此種架構相當於一個小型的神經網路，其中包含了使用1x1、 3x3、5x5 Filter 的卷積層以及最大池化層，且各層在經過運算後合併結果再經過激勵函 數運算後輸出給下一個Inception 架構，此種作法使的神經網路開始往橫向拓展，不同於 先前不斷加深的深經網路，此種網路是向橫加寬。Inception 的架構於2014 年被提出時 稱之為Inception V1，之後便不斷地改良，於2015 年推出兩個更新版Inception V2、 Inception V3，而2016 年則提出最新的Inception V4。 2015 年出現的ResNet[12]則是結合了VGG 和NIN 的概念所產生出的新的神經網 路，ResNet 在神經網路的最後一層使用了全局平均池化(Global Average Pooling)取代了 原先最常被使用的Fully Connected Layer，因為此緣故使ResNet 雖然深度比VGG 網路 來得深，但其權重值卻遠比VGG 網路來的小。 除了分類問題外，物件檢測問題也開始受到關注，2014 年CNN 特徵範圍(Regions with CNN features)[13] ，及2015 年空間金字塔池化網路(Spatial Pyramid Pooling Networks)[14]的提出，藉由先框選出影像中的可能特徵範圍後再進行特徵提取的概念， 使CNN 技術變得更加進步。根據以上CNN 演進的說明，可將CNN 的演進整理如圖四 所示。
