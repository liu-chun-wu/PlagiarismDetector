作為熱門的分類器方法，XGBoost 的中心思維為將多種弱分類器集成一個強分 類器，也就是把多種回歸分類樹混合，每次分裂特徵時就生成一棵樹，這棵樹可 以用不同的方法訓練，代表著每輪特徵分類訓練都是新的一輪函數，訓練完成後 每個特徵分類會在不同的分類樹上有一個分數，相加後就是這個特徵的預測值。 簡要整體流程如下： (1) 每次迭代生成一棵樹。 (2) 每次迭代前，求出上次預測損失函數來改善這一輪。 (3) 根據特徵分裂增益最大化，去生成這棵樹。 (4) 把新的樹添加到原來模型。 而樹的生成為常見boosted tree，優化方式也可以參照生成樹的常見優化手段 [4]。 這樣的方法特色在於相較於其他分類樹，較不易受限於原本的定型適性，就 以本研究為例，Doc2vec 的向量不一定具有高度互斥的特性，那可以預見的是隨機 森林沒辦法有最佳的模型評價，但在XGBoost 的情況下，特徵分裂還會有生成樹 的環節再一次分群，以及全局評估的特性，可以讓分類樹的效果再次提升，提高 分類樹模型對空間向量的適性，這是XGBoost 選為實驗方法最大的原因。
