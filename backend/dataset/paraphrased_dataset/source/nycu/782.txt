特徵的產生主要分成醫療基礎的特徵挑選和機器學習的特徵挑選方法。首先是醫療 特徵，在文獻方面主要收錄Compton et al. (2000)與Sjo (2012)兩個對結腸直腸癌提出預 後因子分析的回顧，我們提取這些與SEER 資料相符合的欄位進行分析；再來針對NCI 與TCOG 兩個癌症協會所歸納出的預後因子，以及醫師在評估病患時所參考的預後因子 共五組預後因子作為醫療基礎的預後因子。 其次是機器學習的特徵挑選方法，主要採用Wrapper 的挑選方式，使用的核心分類 器為Extra-Tree，特徵重要性的評估方法與隨機森林相同，根據每個特徵進行重要性排 序，這表示這些特徵對於目標值的影響程度。詳細的運作流程我們將在3.4.2 與3.4.3 進 行介紹。 醫療基礎的預後因子 本研究根據2.1.2 小節所做的文獻探討，取出過去醫療領域研究中的特徵，在與 SEER 資料集進行欄位的比對，取出交集的欄位進行存活率預測。 機器學習特徵挑選 在機器學習的特徵挑選上主要有三種方法，分別是包裝法（Wrapper）、過濾器（Filter）、 嵌入法（Embedded）(Guyon & Elisseeff, 2003)。Wrapper 使用機器學習模型作為評分的 黑盒子，根據不同特徵子集的預測能力進行評分，常見的方法有向前選擇（Forward Selection）和向後剃除（Backward Elimination）；Filter 則把挑選步驟作為前處理的階段， 選擇與目標相關的特徵，與模型的選擇無關；Embedded 則是在建立模型時就會自行挑 選特徵，例如：決策樹的資訊獲利或正則化方法（Regularization） Wrapper 是一種以模型為基礎的特徵挑選方法，概念非常單純，主要有輸入的資料 與特徵、機器學習之模型以及輸出評估分數，而評估分數可以是回歸的MSE 或分類的 準確率，其主要運作步驟如下。 首先將選擇一個特徵與所有資料送入機器學習的模型當中，其輸出會產生出一個評 估分數，可以是回歸MSE 也可以是分類Precision，取決於原始資料的目標值，接著會 將下一個特徵與前一組已產生分數的特徵進行結合，再次計算出評估分數，一直持續進 行上述動作，直到所有的特徵都被訓練到為止，最後將這些評估分數進行比較，擁有最 高評估分數的模型，其內部的特徵組為我們的最佳特徵，也就是屬於這個模型最佳的預 後因子。其概念如圖3-2(Bouaguel, 2015)。 圖3-2、Wrapper 特徵挑選架構圖 產生特徵子集合 機器學習演算法 所有特徵 最佳特徵組 機器學習演算法 Wrapper 框架 評估準確度 特徵重要性 隨機森林(Breiman, 2001)的特徵重要性是在建立模型時，有一個非常重要的概念即 為袋外數據（Out-of-Bag, OOB），其概念就是在隨機森林進行子集合樣本取樣時採用取 後放回的方式，這種方式稱作Boostraping，對於單一決策樹而言，並不會將所有樣本丟 入訓練模型中，會有留在外部的資料，也就是沒被訓練模型使用到的樣本。接著可以使 用OOB 計算每一個決策樹的誤差率，即將這些在未被納入訓練模型的資料進行測試， 並去計算分類錯誤與樣本總數的比例，而產生的錯誤率我們稱作OOB Error。 而隨機森林運用了OOB Error 的概念來進行特徵值權重的排序，減輕數據在排列組 合後還需要進行重新訓練的資源成本，並能針對每一個特徵的重要性進行評估，若所加 入的雜訊會使誤差變大，即代表原始的特徵具有強大的影響力。其評估流程如下。 1. 將每一個決策樹的OOB 進行OOB Error 的計算，我們紀錄為Error 1。 2. 再來針對這些OOB 的原始特徵X 加入一些其他特徵（雜訊），然後再次計算 OOB Error，我們紀錄為Error 2。 3. 若所建立的決策樹共有N 棵，其特徵X 的重要性如公式（3） 𝐼𝑚𝑝𝑜𝑟𝑡𝑎𝑛𝑐𝑒(𝑥) = ∑Error2−Error1 Ntree （3） 在上述的公式中，若Error 2 所提升的數值越高，代表加入其他特徵及雜訊後錯誤 率提高，換言之代表原始特徵X 重要性較比後續加入的特徵還要來得高，相反的如果 Error 2 的數值不變或變化幅度非常小代表原始特徵X 的重要度相對地就沒那麼高了， 之後針對每一棵決策樹進行OOB Error 的計算，就能針對所有特徵進行重要性的評估並 進行排序。
