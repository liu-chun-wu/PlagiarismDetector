本研究運用監督式學習進行預測建模，採用Python機器學習開源套件，包含表3-4中列出的套件[18]以及Keras深度學習模組。開發環境為Jupyter Notebook，版本資訊如圖3-6所示。模型選取流程如下: 使用GridSearchCV最佳化超參數，以MAE評估模型，並根據RMSE與MAE的總和選取最佳模型組合(SD15, SD16)、(SD16, SD17)和(SD17, SD18)。最終模型比較(RMSE + MAE)則使用最佳模型組合與對應的最佳超參數。


以下說明各模型：

線性回歸：基本回歸方法，假設變數間存在線性關係，著重模型偏差。脊回歸則著重模型方差[30]。線性回歸預測自變數(x)與依變數(y)之間的線性關係，簡單線性回歸使用單一自變數，多元線性回歸則使用多個自變數，預測品質取決於變數間關係強弱[31]。脊回歸：改良的線性回歸，用於處理多元線性回歸中輸入變數的共線性或近似共線性問題，也稱為Tikhonov正則化[32][33]。共線性不必然對回歸結果造成負面影響，因此移除變數需謹慎[34]。

決策樹：常用於分類與回歸，具備高解釋性及易用性，可轉換為規則，能處理名義變數和數值變數，不受變數尺度影響。但績效可能較低[35]。決策樹可處理錯誤值和缺值，但需避免過度生長及過擬合問題[36]。

隨機森林：由多棵決策樹組成，透過投票決定最終結果。適用於回歸的隨機森林基於隨機向量生長樹，並利用大數定律避免過擬合[37]。然而，過多決策樹會增加計算成本。重要參數包含ntree(樹的數量)和mtry(候選變數數量)[38]。過多ntree可減少過擬合，但過多mtry或終端節點也可能導致過擬合[39]。

支援向量回歸：已應用於各種銷售預測研究，如電腦主機板、電腦代理商、煙草銷售[40][41][42]以及時尚產品[17]，也是時間序列分析的核心方法[43]。

梯度提升回歸(GBR)：監督式學習模型，具備多種超參數和損失函數(ls, lad, huber, quantile)[44]，因此更具彈性且預測更準確。使用損失函數評估模型效能，沿梯度方向下降以提升效能。能處理異質資料，但訓練較費時，預測則迅速[45]。在Kaggle競賽中表現優異[46]。

極度梯度提升回歸(XGBRegressor)：監督式學習模型，同樣具備多種超參數和損失函數，在鋼筋混凝土板衝剪抗力預測研究中表現優於ANN和隨機森林[47]，也常應用於Kaggle競賽。

輕量梯度提升(LightGBM)：高效的梯度提升樹，結合決策樹和提升算法[48]。與XGBoost主要差異在於使用直方圖算法加速訓練，降低記憶體消耗，並採用leaf-wise策略生長樹。直方圖算法將連續值轉換為離散值，建立特徵值直方圖，降低記憶體消耗而不影響準確性[49]。在水分蒸散量預估研究中表現優於M5Tree和隨機森林，並在手勢識別研究中表現出色[50]。

類神經網路(ANN)：由相互連接的處理單元(神經元)組成，每個連接都具有權重。神經元分層組織，接收輸入並輸出至下一層。權重連接使模型能學習變數間關係。單個前饋類神經網路包含輸入層、單個或多個隱藏層和輸出層。隱藏層和輸出層的神經元使用激活函數。本研究使用兩個隱藏層[51]，儘管增加隱藏層可提升效能，但也可能增加計算時間和過擬合風險[52]。模型結構如圖3-7所示。
