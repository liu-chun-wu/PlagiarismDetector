9DoF SE-YOLO是一個單階段偵測網路，我們引入對齊偵測模組 (ADM) 後，使其具備兩階段偵測和單次輸出。第一階段偵測針對2D影像平面物件，利用預先定義的單一長寬比錨框(anchor box) pw, ph 進行預測。錨框的長寬比透過K-means於訓練集上學習而得，類似YOLOv3，但YOLOv3使用五種不同長寬比。網路預測tx, ty (物件中心點偏移量)，tw, th (預測框長寬比例) 以及置信度to。結合錨框資訊及預測值，得到學習到的錨框(learned anchor)。此階段輸出包含預測框中心點(x, y)和寬高(w, h)。第二階段偵測與原架構相同，針對3D空間物件，預測tx, ty, tz (3D中心點偏移量), tw, th, tl (3D框尺寸比例) 以及旋轉姿態(以四元數 qt , 1qt , 2qt , 3qt 表示)。此階段輸出包含3D中心點(x, y, z), 物體尺寸(w, h, l) 以及旋轉姿態(q0, q1, q2, q3)。
