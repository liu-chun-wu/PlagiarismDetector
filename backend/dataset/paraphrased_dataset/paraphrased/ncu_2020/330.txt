本節介紹3D物件偵測及姿態估計資料集，並闡述修改後的9DoF SE-YOLO架構如何進行9D物件姿態估計。

首先，我們使用NVIDIA提供的「墜落的物品(Falling Things)」資料集進行訓練。該資料集包含RGB和深度影像、相機內部參數，以及物品的類別、3D邊界框和姿態標記，涵蓋20種常見的廚房物品。

接著，9D物件姿態估計流程如下：

a.  輸入RGB-D影像：將對應的RGB-D影像以批次方式輸入卷積神經網路。
b.  Darknet-53提取特徵：融合RGB-D影像資訊並提取特徵。
c.  預測2D物件資訊：利用骨架特徵進行卷積，預測物件在圖像平面中的2D位置、大小和類別，並據此調整錨框。
d.  提取深度影像特徵：為獲取更佳的空間資訊，提取原始深度影像並進行卷積以降維和提取有用特徵。
e.  特徵並聯：將骨架特徵與深度影像特徵並聯，再由感興趣區域卷積(RoI convolution)提取RoI特徵。
f.  特徵對齊及提取：將調整後的錨框位置和大小資訊與並聯後的特徵輸入RoI卷積，進行特徵對齊和提取。
g.  預測物件9DoF和類別：對RoI卷積提取的RoI特徵進行1x1卷積，迴歸出物體在現實世界的大小、位置、姿態和類別。
h.  非最大值抑制：排除同一物件在相同位置的多餘預測邊界框，僅保留最佳結果。
