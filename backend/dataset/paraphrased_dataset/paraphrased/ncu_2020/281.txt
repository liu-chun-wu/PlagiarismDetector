在自然語言處理領域，文字分類、翻譯等任務需要將文字轉換為電腦可理解的數值形態。主要有兩類轉換方法：Bag-of-Words 和 Distributed Representations of Words。Bag-of-Words 將每個詞表示成唯一數值，源於Harris，是較傳統的詞向量表示法。Distributed Representations of Words 則源於Firth，強調理解詞義需考慮其上下文，更重視詞間關係。Bag-of-Words 簡單快速，但詞編碼間缺乏關聯性，詞彙量大時向量冗餘且浪費記憶體。而基於連續數值向量表示詞的方式，能表示詞間語意相似度，改善詞關聯性，避免維度爆炸。常見的 Distributed Representations of Words 模型有 Word2Vec 和 Glove。Savigny 的研究顯示，Word2Vec 在電影評論情緒分類上的準確率優於 Glove 和 Doc2Vec。因此，本文採用 Google 於 2013 年提出的 Word2Vec，它透過學習大量無標籤文本資料，將詞用數學向量表示語意，使語意相似的單字在空間中距離更近。Word2Vec 有 CBOW 和 Skip-gram 兩種架構。CBOW 用上下文預測中間詞，Skip-gram 用中間詞預測上下文。Tomas Mikolov 認為 CBOW 適合短文本和大量訓練資料，訓練速度也較快。考慮到 YouTube 文字特徵較短，本文採用 CBOW 架構訓練 Word2Vec 模型。
