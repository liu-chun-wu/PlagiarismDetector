研究預測全球資料產出量將從2018年的33ZB增長到2025年的175ZB。隨著資訊科技演進，資料處理方法從單機發展到Apache Hadoop的分散式計算，使我們得以處理電商網站使用者行為資料、機台產出log等大量資料，進而提供更精準的分析。然而，Hadoop每次MapReduce計算都需進行檔案存取，導致運算效能不佳。2009年，加州大學柏克萊分校AMPLab公開了基於記憶體儲存的分散式平行運算框架Apache Spark，大幅改善了此情況。Spark在記憶體中計算的特性使其更適合用於多次迭代的資料分析工作，效能可達Hadoop的100倍。此外，Spark對程式語言和資料類型的支援也相當彈性，使其迅速普及。本研究將使用Spark MLlib中的ALS演算法，相較於以Hadoop MapReduce為框架的Apache Mahout，其效能表現更佳。
