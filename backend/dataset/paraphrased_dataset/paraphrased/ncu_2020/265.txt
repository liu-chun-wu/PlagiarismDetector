傳統長短期記憶模型缺乏針對特定關注部分提取語義的能力，因此Li Zhao提出了結合注意力機制的長短期記憶模型 (AT-LSTM)。該模型將觀點詞向量與每個 LSTM 隱藏層輸出連接，藉由注意力機制計算各隱藏層輸出在語義上的權重，最終合成語義代表向量。其計算公式如下：

M = tanh ([𝑊ℎ𝐻; 𝑊𝑣(𝑣𝑎⊗𝑒𝑁)]) 

α = softmax(𝑤𝑇M) 

r = H𝛼𝑇

其中，H 代表隱藏層輸出，va 代表觀點詞向量，⊗表示克羅內克積，  𝑣𝑎⊗𝑒𝑁 代表將 va 向量複製 N 次形成矩陣，α 代表注意力權重向量，r 代表在給定觀點詞彙下句子的加權表示，Wh 與 Wv 為權重矩陣。AT-LSTM 架構中，N 為句子長度，w 為詞向量，h 為 LSTM 各隱藏層輸出向量，va 為觀點詞向量，α 則為各隱藏層輸出在最終語義向量中的權重比例。
