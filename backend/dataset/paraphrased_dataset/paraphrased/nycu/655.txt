隨著人工智慧與機器學習日益普及，仰賴機器自動化執行策略的現象也隨之增加，伴隨而來的是對機器執行策略的中立性、公平性及倫理道德的關注。人們常直覺認為，基於數學計算和推導的機器學習應是中立公平的，然而，機器學習的數據源自人類日常生活，包含對話、影像、行動數據等，因此也承襲了人類生活、知識系統及文化中的偏見和刻板印象。

機器學習中的「偏頗」(Bias)指先驗資訊，是過去資訊累積的經驗所提供的機率分佈，也是智能體必要的先備知識。沒有偏頗的機器學習模型等同於沒有學習到任何知識，其預測結果並不可信。然而，基於不公正數據源的「偏頗」會形成「有害的偏頗」(此處稱為「偏見」)，由於數據僅呈現相關性事實，不涉及道德人權也無法說明因果關係，「有偏見的演算法」可能在職場、執法等應該公平的場域，僅因膚色、性別等因素產生差異性的預測結果。

語音辨識的性別偏見早在2002年就被提出，女性聲音的辨識率通常低於男性，醫療用聲控指示系統在男性操作時表現更佳。Google 廣告也更常將高薪工作鎖定男性投放。犯罪預測演算法經常鎖定黑人多於白人，可能助長種族歧視的執法行為。臉部辨識的種族歧視也是一個著名例子，Google 的相片應用程式曾因將黑人辨識為大猩猩而受指責。研究指出，Microsoft、IBM 和 Face++ 的臉部辨識系統中，深膚色人種和女性的辨識率都低於淺膚色人種和男性。
