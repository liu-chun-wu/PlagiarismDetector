序列到序列模型將輸入序列編碼成單個語意向量（Context），包含原始序列的所有信息。然而，長輸入序列可能導致Context無法容納所有特徵，從而降低解碼效果。注意力機制可以增強解碼器輸出層對輸入向量的關注，本文介紹基本的Luong Attention機制。Luong等人（2015）提出此機制，Weng等人（2018）也用其強化序列到序列模型進行英文語音識別。

Encoder狀態ℎ̅𝑆和Decoder狀態ℎ𝑡 (均為[𝑛ℎ, 1]向量) ，ℎ̃𝑡表示注意力機制層輸出的最終Decoder狀態。Decoder在t時刻隱藏層狀態ℎ𝑡對Encoder每個隱藏層狀態ℎ̅𝑆的權重𝑎𝑡(𝑠)計算如下：

𝑎𝑡(𝑠) = exp (𝑠𝑐𝑜𝑟𝑒(ℎ𝑡,ℎ̅𝑠)) / ∑ exp (𝑠𝑐𝑜𝑟𝑒(ℎ𝑡,ℎ̅𝑠′))  (𝑠′)

公式中的score計算方式有三種：

score(ℎ𝑡, ℎ̅𝑠) = 
1. ℎ𝑡 𝑇ℎ̅𝑠 (Dot)
2. ℎ𝑡 𝑇𝑊𝑎ℎ̅𝑠 (General)
3. 𝑣𝑎 𝑇tanh (𝑊𝑎𝑐𝑜𝑛𝑐𝑎𝑡(ℎ𝑡, ℎ̅𝑠)) (Concat)

Dot 使用向量內積，General 通過權重矩陣𝑤𝑎([𝑛ℎ, 𝑛ℎ])計算，Concat 使用激勵函數計算。

利用權重𝑎𝑡(𝑠)計算加權後的Context狀態向量𝑐𝑡([𝑛ℎ, 1]向量)：

𝑐𝑡= ∑𝑎𝑡(𝑠)ℎ̅𝑠 (𝑠)

將𝑐𝑡與原始Decoder隱藏層狀態ℎ𝑡合併，經激勵函數得到ℎ̃𝑡：

ℎ̃𝑡= tanh (𝑊𝑐∙𝑐𝑜𝑛𝑐𝑎𝑡(𝑐𝑡, ℎ𝑡))

𝑐𝑡和ℎ𝑡均為[𝑛ℎ, 1]向量，使用全連接層𝑊𝑐防止Decoder cell過大。

最後，將經過注意力機制處理的ℎ̃𝑡乘以輸出權重矩陣𝑊ℎ𝑜，加上偏置𝑏ℎ𝑜得到t時刻的輸出𝑦𝑡：

𝑦𝑡= 𝑊ℎ𝑜ℎ̃𝑡+ 𝑏ℎ𝑜
