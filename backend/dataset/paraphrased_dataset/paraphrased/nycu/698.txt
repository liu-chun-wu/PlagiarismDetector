支持向量機 (SVM) 基於結構風險最小化原則，在有限樣本中寻求模型泛化能力的最佳化。結構風險最小化 (SRM) 试图同时最小化经验风险和 VC 維 (Vapnik-Chervonenkis dimension)。该理论最初由 Vapnik 及其同事在 AT&T 贝尔实验室研究二元分类问题时提出。SVM 学习算法善于识别复杂数据中的细微模式，通过学习样本进行判别分类，预测未知数据的类别。SVM 主要分为支持向量分类 (SVC) 和支持向量回归 (SVR)。

传统的统计/回归方法侧重于最小化所有训练样本的预测值与实际观测值之间的偏差，而 SVR 则关注最小化广义误差界限以提高泛化能力。广义误差界限结合了训练误差和正则化项，用以控制假设空间的复杂度。Vapnik、Steven Golowich 和 Alex Smola 于 1997 年提出了 SVR，作为 SVM 在回归问题中的应用。SVR 是 SVM 的一种广泛应用形式，是支持向量在回归和函数估计中的推广。

假设训练数据表示为 (𝑥₁,𝑦₁),..., (𝑥ₗ,𝑦ₗ) ∈ Rᵈ × R，其中 x 表示输入特征，y 表示对应的回归值（类似于 SVM 中的目标类别）。令 f(𝑥) = 𝑤∙𝑥 + 𝑏，w ∈ Rᵈ, b ∈ R，如果对于每个样本 xᵢ，f(𝑥ᵢ) 与 𝑦ᵢ 的差值都很小，则 f(𝑥) 可以准确地预测 y，而 SVR 的目标就是找到这样的平面 w。其目标函数为：min 1/2‖𝑤‖²  subject to ‖𝑦ᵢ - (𝑤∙𝑥ᵢ - 𝑏)‖ ≤ ε，其中 ε ≥ 0 表示 SVR 预测值与实际值的最大允许偏差。

在 ε 合理的情况下，如果上述优化问题有解，则称之为可行。然而，实际应用中由于噪声和误差等因素，通常情况下不可行，因此需要引入松弛变量 ξᵢ 和 ξᵢ*，允许部分样本落在 ε-管道之外。新的目标函数包含惩罚参数 C，用于平衡模型复杂度和训练误差，防止过拟合或欠拟合。由于本研究数据集呈回归分布，因此采用 SVR 进行实验分析。
