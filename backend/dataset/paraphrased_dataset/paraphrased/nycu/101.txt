早期中文特徵擷取分類器實驗常用tf-idf搭配KNN或SVM，但本研究包含數值欄位，因此採用在混合資料表現較佳的隨機森林[14]。

Kaggle競賽中，XGBoost因訓練時間短、準確率高，且不受固定欄位長度限制，成為混合資料分析的熱門選擇，其效能優於隨機森林，是分類樹模型的優良選項，可與隨機森林和類神經網路方法比較。

本研究重點之一是測試類神經網路效益，探討預處理方法與類神經網路的適配性。這對於評估導入機器學習、尤其是類神經網路模型的機構至關重要，是影響模型優化和精度提升的關鍵因素。因此，本研究也採用MLP作為研究方法，比較不同預處理方法對其效能的影響。
