In English, words are separated by spaces, allowing spaces to be used as word delimiters. This study uses the nltk.wordpunct_tokenize API for word tokenization.  Figures 5 and 6 show the top 35 most frequent words in the 'title' and 'text' fields, respectively, after tokenization.  These figures reveal a significant number of stop words.  If these stop words are not removed, they may reduce accuracy and increase dimensionality in subsequent modeling, leading to longer training and testing times.
