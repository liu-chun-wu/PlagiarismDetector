Hinton於1986年首次提出分佈式表示（Distributed representation）的概念。與高維度的整數向量獨熱表示相比，分佈式表示通常是較低維度的實數向量，由高維度獨熱表示經前饋類神經網路壓縮而成。

2003年，Bengio等人利用N-gram模型和類神經網路訓練語料庫中的單詞序列，得到各個單詞的分佈式表示，其模型架構藉由序列中單詞的前後字彙來訓練該單詞的分佈式表示。分佈式表示可以視為濃縮後的獨熱表示，去除冗餘維度，使維度利用最大化，並進一步解決0/1表示在自然語言處理中的問題。Bengio的研究指出，分佈式表示可以減少維度詛咒，提高語言模型的泛化能力，使其能更好地預測未出現的單詞序列。

現今的詞嵌入（Word Embedding）即指這種單詞的分佈式表示法，通常由語料中的句子經類神經網路訓練而成。詞彙之間的關係可以通過計算詞向量之間的餘弦相似度或歐式距離得到，詞嵌入可以理解為單詞抽象概念在數學空間中的表示，也可稱為詞表示（Word representation）。

由於傳統類神經網路模型訓練時間過長，Tomas Mikolov等人於2013年提出了連續型連詞袋模型（CBOW）和跳躍式模型（Skip-Gram），並使用階層軟式最大化（HS）和負例採樣（NS）方法，提供了一個名為word2vec的工具包，包含數種詞嵌入方法，其特點是訓練速度快，且得到的詞表示具備類比性質，例如「巴黎之於法國，相當於羅馬之於義大利」。這種類比關係在詞向量運算中大致滿足「A-B=C-D」的關係。Tomas Mikolov認為，具備類比關係的詞嵌入能捕捉詞彙的語意結構，表示詞嵌入品質較佳，因此類比關係的強弱也常用於衡量詞嵌入訓練的優劣。

後續研究例如GloVe、以及以詞性標註、短語識別、命名實體識別和語義角色標註等任務為目標的預訓練詞嵌入方法，都對詞嵌入訓練做出了進一步的改進。
