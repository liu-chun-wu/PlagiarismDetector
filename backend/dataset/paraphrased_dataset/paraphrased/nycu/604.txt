決策樹是一種預測模型，它描繪對象屬性與對象值之間的映射關係。樹中每個節點代表一個對象，每個分叉路徑代表一個可能的屬性值，每個葉節點則代表從根節點到該葉節點路徑所表示的對象值。決策樹只有單一輸出，需要多個輸出時，可以構建多個獨立的決策樹。

在數據挖掘中，決策樹是一種常用技術，可用於數據分析和預測。其主要結構如下：根節點包含所有訓練數據；節點表示一個測試屬性，根據不同分割準則分割數據；分支連接節點，代表一種分割準則；葉節點是終端節點，不再分支。

決策樹的目標是找到具有同質性子節點的分裂點，使其不斷延展，產生更多同質節點。為分類輸入數據，每個節點都是一個判斷式，根據某個變數判斷輸入數據是否大於、等於或小於某個數值，從而將數據分成若干類。決策樹的優點是可以進行分類，同時處理連續和類別數據，並在預測和分類時明確指出變數的重要性。

分類迴歸樹 (CART) 兼具分類和迴歸功能，由美國統計學家 Brieman 於 1984 年提出。其特點是分類時一次產生兩個節點，且應變數和自變數類型不限，分析更具彈性，是一種常用的決策樹方法。CART 根據預測變數及其相對指標，將訓練樣本劃分為數個已知類別，並將劃分過程總結成一系列規則。

CART 的分析流程大致分為三個步驟：將樣本數據劃分為訓練樣本和測試樣本，並用訓練樣本構建最大決策樹；根據修剪準則，從樹的底部向上修剪，產生子樹群，直到滿足修剪準則；使用測試樣本進行交叉驗證，從子樹群中選擇最佳樹狀結構。樹狀結構建成後，CART 會產生一系列分類規則，幫助決策者理解分類過程。
