循環神經網路 (RNN) 於 1980 年代提出，與前饋神經網路 (FNN) 的主要差異在於 RNN 能學習序列資訊。本文選用 RNN 架構處理序列資料，相關架構可參考 Chung, Gulcehre, Cho, & Bengio (2014)，Jozefowicz, Zaremba, & Sutskever (2015) 以及 Greff, Srivastava, Koutník, Steunebrink, & Schmidhuber (2016) 等論文。

RNN 的關鍵在於其隱藏層能儲存歷史資訊，類似人腦記憶。RNN 廣泛應用於機器翻譯、語音識別和文字識別 (OCR) 等領域。RNN 是一種具有「記憶力」的神經網路，其模型可以簡化為：輸入值 (𝑋𝑡)、隱藏層 (ℎ𝑡)、輸出值 (𝑂𝑡) 以及隱藏層輸出到下一個隱藏層的權重 (w)。RNN 與 FFNN 最大的不同在於其迴圈結構。

RNN 的演算法可以表示為：

𝑎𝑡 = ℎ𝑡−1𝑊 + 𝑋𝑡𝑌 + 𝑏𝑖  (17)
ℎ𝑡 = tanh(𝑎𝑡)  (18)
𝑂𝑡 = ℎ𝑡V + 𝑏𝑜  (19)

其中，𝑎𝑡 為隱藏層對輸入的運算，tanh 為激勵函數 (也可使用 Sigmoid 或 ReLU)，𝑊、𝑌 和 V 分別為隱藏層到隱藏層、輸入到隱藏層以及隱藏層到輸出的權重，𝑏𝑖 和 𝑏𝑜 為誤差值。

時間點 t 的隱藏層受 t-1 的影響，並影響 t+1 的輸出，賦予 RNN 類似記憶的機制，使其在處理自然語言等具有順序關係的資料時特別有用。

然而，RNN 的缺點是輸入和輸出長度必須固定。此外，RNN 使用反向傳播法進行優化時，若時間序列很長且權重小於 1，會出現梯度消失問題，導致模型訓練失敗。公式 (20) 展示了此現象：

ℎ𝑡 = 𝑊ℎ𝑡−1 + 𝑌𝑋𝑡 + 𝑏
ℎ𝑡 = (𝑊²ℎ𝑡−2 + 𝑊𝑌𝑋𝑡−2 + 𝑊𝑏) + 𝑌𝑋𝑡 + 𝑏) …
ℎ𝑡 = (𝑊𝑡ℎ0 + ⋯) + 𝑌𝑋𝑡 + 𝑏)  (20)

下文將提出解決此問題的新方法。
