YOLO，一個2018年提出的物件偵測神經網路架構，在當時的兩個資料集[17][40]中達到領先水準，並實現了即時偵測。本文介紹的YOLOv3，承襲了前代方法，並主要透過兩種方式改良網路架構：一、使用ResNet[32]中的殘差網路設計更深的網路；二、參考FPN架構設計，加入多尺度預測及多尺度特徵融合。

YOLO採用one-stage方法，直接從圖片中偵測邊界框。它將圖片等分成S×S個區塊（如圖13所示），每個區塊預設N個anchor box。神經網路預測每個anchor box的長寬變化、中心位置位移和所屬類別。網路估計結果與邊界框的關係如下列公式所示：

𝑏𝑥= 𝜎(𝑡𝑥) + 𝑐𝑥 (4)
𝑏𝑦= 𝜎(𝑡𝑦) + 𝑐𝑦 (5)
𝑏𝑤= 𝑝𝑤𝑒𝑡𝑤 (6)
𝑏ℎ= 𝑝ℎ𝑒𝑡ℎ (7)

其中，𝑡𝑥, 𝑡𝑦, 𝑡𝑤, 𝑡ℎ為網路估計值；(𝑏𝑥, 𝑏𝑦)為邊界框中心位置；(𝑏𝑤, 𝑏ℎ)為邊界框的寬與長；(𝑝ℎ, 𝑝𝑤)為anchor box的寬與長；(𝑐𝑥, 𝑐𝑦)為anchor box所在區域左上角的位置；𝜎()代表sigmoid函數。

圖13：圖片等分S×S個區塊示意圖，圖片出自[42] 第四章、問題描述及系統架構
