研究初期使用Jieba分詞處理10,666篇文本及摘要，產生1,010,355個詞彙的字典，但模型訓練耗時且摘要效果不佳。參考Hu et al. (2015)的單字分詞方法，最終採用訓練文本和摘要中出現頻率較高的6,588個字，並加入<start>、<end>、<nu>、<mask>等特殊符號，組成6,592個字的字典。這種方法提升了訓練速度和摘要效果。  字典中的每個詞都賦予索引並使用one-hot編碼初始化詞向量，例如："我"的索引為0，對應的向量為[1, 0, 0, 0, ..., 0]，以此類推。
