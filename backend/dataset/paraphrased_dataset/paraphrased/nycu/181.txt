在自然語言處理中，尤其是搜尋引擎應用，句子的分割粒度至關重要，它直接影響召回率和精準度。以“大不列顛及北愛爾蘭聯合王國”為例，可切分為“大不列顛及北愛爾蘭/聯合王國”或更細粒度的“大不列顛/及/北愛爾蘭/聯合/王國”，甚至“聯/合/王/國”。

大粒度詞彙表義能力強，適合做關鍵字或標籤，使用大粒度詞建構倒排索引能提高搜尋精準度。但這會犧牲召回率，例如，以“大不列顛及北愛爾蘭聯合王國”為關鍵字搜尋，無法檢索到包含“北愛爾蘭王國”的句子，而使用更細粒度的“大不列顛”、“北愛爾蘭”、“聯合王國”則可以。因此，中文分詞器需要根據不同應用場景調整分詞策略。

中文分詞的另一難點是確定“基本粒度詞”。古漢語中，“己所不欲，勿施於人”每個字都是獨立的詞，而現代漢語則常用雙音節詞彙解釋這些單字詞。此外，某些詞彙細粒度切分會產生無意義的詞，例如“老/虎”和“水/果”中的“老”和“水”，雖然有其本身含義，但與原詞無關。這些都增加了分詞模型的處理難度。

目前尚無公認的粒度標準，常用的評測語料庫如北大pku-test、微軟亞洲研究院msr-test、人民日報標註語料等，其切分標準也各不相同。近年來，BERT等預訓練模型的出現，通過詞嵌入技術利用大量文本數據，有助於緩解粒度選擇的難題。
