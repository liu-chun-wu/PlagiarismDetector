XGBoost的核心思想是集成多個弱分类器（回归分类树）形成一个强分类器。每次分裂特征都会生成一棵新的树，并以不同的方法训练，意味着每轮特征分类训练都是一个新的函数。训练完成后，每个特征在不同的分类树上获得一个分数，这些分数相加得到最终的预测值。

其简要流程如下：

1. 每次迭代生成一棵树。
2. 每次迭代前，计算上次预测的损失函数以改进当前迭代。
3. 基于特征分裂增益最大化生成树。
4. 将新生成的树添加到现有模型中。

树的生成采用常见的boosted tree方法，优化方式也参考了常见的树生成优化手段。

相比其他分类树方法，XGBoost不易受限于数据的特性。例如，即使Doc2vec向量不具备高度互斥性，导致随机森林模型效果不佳，XGBoost仍然可以通过特征分裂和树生成环节再次进行分群，并结合全局评估的特性提升分类效果，提高分类树模型对空间向量的适应性。这也是选择XGBoost作为实验方法的主要原因。
