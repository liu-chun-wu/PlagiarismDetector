This paper extends the use of Non-negative Matrix Factorization (NMF) to discover latent user preferences and item characteristics by decomposing latent matrices. Given a data matrix R (m x n), user latent preferences are represented by vectors  pi (d-dimensional) and item characteristics by vectors qj (d-dimensional).  P (m x d) and Q (n x d) represent the matrices of all user preference vectors and item characteristic vectors, respectively.  Typically, d is smaller than m or n, resulting in smaller P and Q matrices compared to R. NMF factorizes the positive matrix R into non-negative matrices P and Q. The objective is to minimize the Frobenius norm of the difference between R and the product of P and Q transpose: L = ||R - PQᵀ||F. While L is convex in P or Q individually, it is not convex in both simultaneously, making finding a global minimum challenging.  An iterative update algorithm by Lee & Seung is used to find a local minimum: p_ij^(t+1) = p_ij^t (RQ)_ij / (PQᵀQ)_ij and q_ij^(t+1) = q_ij^t (RᵀP)_ij / (QPᵀP)_ij. Since d is typically much smaller than m and n, NMF provides a compressed approximation of R: R ≈ PQ.  Each column vector of R can be approximated as a linear combination of the columns of P, weighted by the components of Q.  Thus, P can be viewed as a basis optimized for linearly approximating the data in R.  Because fewer basis vectors in P represent many data vectors in R, effective approximation requires these basis vectors to capture the latent structure within the data.
