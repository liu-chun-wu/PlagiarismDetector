遞迴網路（RNN）廣泛應用於語言建模（Sutskever et al., 2011; Graves, 2013; Hermans & Schrauwen, 2013）和機器翻譯（Bahdanau, Cho & Bengio, 2014）。RNN 的記憶區塊能儲存序列資訊是其廣泛應用的關鍵。然而，簡單RNN難以訓練（Bengio et al., 1994; Pascanu et al., 2013），因此出現了LSTM（Hochreiter & Schmidhuber, 1997）和GRU（Cho et al., 2014）等改進模型。許多研究致力於優化RNN的結構和訓練方法（ElHihi & Bengio, 1995; Koutnik et al., 2014; Merity et al., 2017）。Pascanu et al. (2014)比較了不同RNN結構在音樂建模和語言建模任務中的表現。Jozefowicz, Zaremba & Sutskever (2015)測試了上萬種RNN變體，認為超越LSTM的結構難以發現。Melis et al. (2018)比較了LSTM、Recurrent Highway Network和NAS，認為LSTM更優。近期重要的RNN結構變形是加入殘差連接（He et al., 2016），提升了LSTM的效能（Pradhan & Longpre, 2016; Prakash et al., 2016; Kim, El-Khamy & Lee, 2017; Wu et al., 2016）。
