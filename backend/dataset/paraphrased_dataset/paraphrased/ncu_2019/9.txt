Graves 探討了深度 LSTM 網路在手寫辨識和文字生成上的優勢。Sutskever 等人提出了一種深度 LSTM 方法，其中所有 LSTM 層的隱藏狀態都全連接。Prakash 等人提出了一種深度 LSTM 模型，其中上一層在時間步 t 的隱藏狀態  ℎ𝑡^(𝑙−1) 會傳遞給下一層的隱藏狀態 ℎ𝑡^(𝑙) ，l 代表層數。第 l 層的啟動函數可以表示為：ℎ𝑡^(𝑙) = 𝑓ℎ^𝑙(ℎ𝑡^(𝑙−1), ℎ𝑡−1^(𝑙))。當 t = 0，l = 0 時，ℎ 由 LSTM 等式計算得出，各層的 h 則由上述公式計算。

Prakash 等人借鑒 ResNet 中使用殘差學習的概念，發展出堆疊殘差 LSTM 網路。深度學習網路層數加深時，會出現退化現象，即模型層數和複雜度增加時，準確度反而下降。這與梯度消失問題不同，殘差網路可以幫助解決退化現象。Kim 等人將殘差連接用於輸出門，而 Prakash 等人提出了四層堆疊殘差 LSTM 網路，殘差每兩層通過逐點加法的方式加入網路，因此需要輸入資料與隱藏狀態 ℎ𝑡 維度相同。

Sutskever 等人將前一層 LSTM 網路的隱藏狀態作為下一層的輸入，而 Prakash 等人堆疊 LSTM 的最後一個隱藏狀態，具有殘差連接的隱藏狀態可以表示為：ℎ𝑡^(𝑙) = 𝑓ℎ^𝑙(ℎ𝑡^(𝑙−1), ℎ𝑡−1^(𝑙)) + 𝑥^(𝑙−𝑛)。在 l 層的 h 是殘差 𝑥^(𝑙−𝑛) 的函數，𝑥^𝑙 代表第 l + n 層的輸入。當 n = 1 時，函數變成一個殘差為輸入值 x 的簡單 LSTM 網路。此時，殘差網路的殘差連接沒有增加可學習的模型參數，也就是模型複雜度沒有增加，因此不一定要將模型的每一層都連接。
