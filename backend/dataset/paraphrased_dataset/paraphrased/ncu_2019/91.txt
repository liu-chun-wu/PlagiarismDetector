中文斷詞是中文自然語言處理的核心技術，也是LDA模型的第一步。常用的結巴斷詞可以加入停用詞，例如在處理筆錄時，可以將“問”、“答”、“報案”、“報案人”、“今日”、“未”、“至”、“的”、“於”等詞加入停用詞列表。

斷詞後，需要將詞彙向量化（Word Embedding），讓電腦通過閱讀大量文本學習詞彙的向量表示，無需語言學知識。深度學習的發展使得詞彙向量可以自動調整，提升電腦的抽象思考能力。詞彙向量在向量空間中，相近詞彙的距離更近。詞彙向量作為深度學習模型中的可修改參數，具備語義自動調整能力。

傳統的one-hot encoding方法會因為詞彙庫增大而導致維度過高，造成空間浪費和處理速度降低，且無法體現詞彙間的關係。Word2Vec是Google在2013年提出的一種詞向量表示方法，它將詞彙映射到低維向量，語義相近的詞彙在空間上距離更近，解決了one-hot encoding的缺點。

Word2Vec包含CBOW和Skip-gram兩種模型。CBOW利用上下文預測中心詞，而Skip-gram利用中心詞預測上下文。例如，以“以...聯繫”為上下文，CBOW可以預測中心詞“LINE”、“電話”、“臉書message”，這些詞都屬於通訊工具。而Skip-gram則可以通過中心詞“LINE”、“電話”、“臉書message”預測上下文“以...聯繫”，表示通訊方式。通過詞彙向量，可以判斷“facetime”也屬於通訊工具，其詞向量與“LINE”、“電話”、“臉書message”相近。
