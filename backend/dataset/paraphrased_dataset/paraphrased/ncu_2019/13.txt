殘差塊 (He et al., 2016) 的輸出為  𝑜 = 𝐴𝑐𝑡𝑖𝑣𝑎𝑡𝑖𝑜𝑛(𝑥 + 𝐹(𝑥))，其中𝐹為轉換函數，𝑥為輸入。這種結構使神經層能學習恆等對應的調整。TCN 的接受域受模型深度𝑛、空洞因子𝑑和濾波器大小𝑘影響，因此深層大型 TCN 模型的穩定性至關重要。例如，預測長度為 212 的高維歷史資料，可能需要 12 層、每層使用不同濾波器提取特徵的神經網路。TCN  使用殘差連接 (圖 6 (b))。其殘差塊包含兩層空洞卷積和非線性函數 (Bai et al., 2018 使用 Relu)，並使用權重歸一化、空洞卷積後加上 spatial dropout  (隨機將某些 filter channel 歸零)。由於 TCN 中輸入和輸出的 channel 數可能不同，標準殘差網路的直接相加方式並不適用。為此，TCN 使用 1x1 卷積確保 element-wise 相加時 channel 數一致 (圖 6(b)、圖 6(c))。
