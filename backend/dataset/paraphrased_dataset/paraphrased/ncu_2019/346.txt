深度學習是機器學習的子領域，利用多層神經網路進行學習，並擁有自動化特徵工程的優勢，使其更容易解決問題 (Chollet, 2017)。傳統機器學習需要事先定義特徵，但在複雜網路中，特徵工程變得困難，而深度學習的自動化特徵工程有效提升了效率。

然而，深度學習的潛力受訓練過程的有效性限制，例如梯度問題，權重設定不當可能導致梯度消失或爆炸，影響結果 (Gers, 1999)。遞歸神經網路 (RNN) 廣泛應用於時間序列，但 RNN 難以處理長期依賴關係。LSTM 的出現改善了此問題，能選擇性保留重要資訊 (Pant, 2017)。Pant 的研究以美元兌盧比匯率預測為例，發現 LSTM 模型雖優於一般模型，但仍有不足，例如低估價格波動。Li (2018) 等人的研究指出，調整或增加輸入資料可提高交易收益。除了 LSTM，時間卷積網路 (TCN) 也具有競爭力，甚至在準確度上超越 LSTM (Bai, 2018)。

啟動函數使神經網路具備非線性表達能力。常見的有 Sigmoid、tanh 和 ReLU。Sigmoid 常用於分類問題；tanh 可能存在梯度消失問題；ReLU 訓練速度快，尤其在深度卷積網路中 (Krizhevsky, 2012)。針對時間序列，SeLu 啟動函數 (Hochreiter, 2017) 具有更好的收斂性，能避免梯度消失和爆炸。本研究在 TCN 模型中使用 SeLu，而 LSTM 模型受限於 Cudnn，使用 tanh。

損失函數用於衡量模型預測與真實值的差距。常用的損失函數有均方差 (MSE) 和平均絕對誤差 (MAE)。MSE 易受異常值影響，MAE 梯度較大，可能陷入局部最佳解。時間序列預測的損失函數通常是非對稱的，其設計會影響預測結果 (Granger, 1999; Lee, 2007)。

時間序列分析需要特定模型，例如 RNN。RNN 具有類似記憶的特性 (Chollet, 2017)，但無用的記憶會成為噪聲。LSTM 能選擇性保留重要資訊，提升效率。GRU 是 LSTM 的變體，簡化了結構，但本研究中使用 Cudnn 在 GPU 上運行，LSTM 和 GRU 效率差異不大。CNN 通常不適用於時間序列預測，因為缺乏因果關係限制。TCN 透過因果卷積，兼具 RNN 和 CNN 的優勢 (Bai, 2018)，因此本研究採用 TCN 模型進行時間序列預測。
