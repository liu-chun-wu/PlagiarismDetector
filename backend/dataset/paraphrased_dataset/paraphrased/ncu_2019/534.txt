倒傳遞演算法是一種基於最陡梯度下降的監督式學習方法，旨在最小化誤差函數。其權重更新公式為：Δ𝑤𝑖,𝑘(𝑛) = −𝜖× 𝜕𝐸/𝜕𝑤𝑖,𝑘 + 𝛼× ∆𝑤𝑖,𝑘(𝑛−1)，其中𝑤𝑖,𝑘代表第k個輸入值到神經元i的連接權重，𝐸代表訓練集上誤差平方的總和，𝑛代表第𝑛次學習回合，𝜖和𝛼分別代表學習速率和慣性因子。學習速率影響網路收斂速度及穩定性，過小會導致訓練時間過長且可能陷入局部最小值，過大則可能造成震盪。慣性因子則用於改善收斂過程中的震盪並加速收斂。

由於計算誤差需要遍歷整個訓練集，對於大型訓練集或線上學習任務效率較低。因此，另一種更新公式為：Δ𝑤𝑖,𝑘(𝑛) = −𝜖× 𝜕𝐸𝑝/𝜕𝑤𝑖,𝑘 + 𝛼× ∆𝑤𝑖,𝑘(𝑛−1)，其中𝐸𝑝代表單個訓練樣本的誤差平方。𝜖和𝛼的選擇對訓練成功與速度至關重要，手動調整參數耗時且困難，但適當的參數調整可以獲得良好效果。


神經網路通常包含輸入層、隱藏層和輸出層。輸入層神經元數量由輸入維度決定，輸出層則輸出計算結果。隱藏層數量和神經元數量需要根據具體任務調整，合適的隱藏層結構可以提高預測效果，但過多的隱藏層可能導致過擬合。
