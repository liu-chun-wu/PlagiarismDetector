相互資訊(MI)衡量特徵變數間的依賴程度。MI越小，表示兩個不同事件空間的隨機特徵變數關聯性越低；MI越高，關聯性越高。若t和ci相互獨立，則t不向ci提供任何資訊，其相互資訊為零。Estevez et al. (2009) 和 Fleuret and Ch (2004) 指出，以相互資訊進行特徵選取對數值型資料集的計算極為敏感，即使微小的估計誤差也會影響特徵選取。Battiti (1994) 使用基於相互資訊的演算法對鳶尾花資料集進行前處理，提升了分類準確度。Wang and Lochovsky (2004) 提出相互資訊可用於識別文本中依賴性較弱的文字特徵，並顯著提升分類器訓練速度。相互資訊計算公式如下 (Battiti, 1994)：

MI(t, ci) = ∑P(ci)log(P(t,ci)/(P(t)P(ci)))
