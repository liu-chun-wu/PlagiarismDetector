Spark是一個快速的大數據叢集分析引擎，源自加州大學柏克萊分校的AMPLab，並以開源叢集計算框架的形式提供。它繼承了Hadoop MapReduce的可擴展性和容錯能力，並加以改進，提供更簡便的編程方式，支援Java、Python、Scala和R等程式語言API。Spark能處理各種複雜計算任務，包括SQL語法執行 (Spark SQL)、串流資料處理 (Spark Streaming)、圖像處理 (GraphX) 和機器學習 (MLlib)，這些以前需要其他工具才能完成。Spark支援多種叢集運作模式：Local、Spark Standalone、Mesos和YARN。它採用類似MapReduce的RDD (Resilient Distributed Datasets) 計算模型。RDD是Spark程式的核心，其計算儲存在記憶體中，不像MapReduce需要透過HDFS溝通，因此Spark的運算速度比Hadoop MapReduce更快，且可在不同平台執行，例如Hadoop YARN、Apache Mesos和Kubernetes，並可存取HDFS、Alluxio、Apache Cassandra、Apache HBase、Apache Hive等多種數據來源，成為現今主流的大數據處理引擎。RDD由一或多個Partition組成，這些Partition是分散在不同節點並儲存在記憶體中的數據分片。RDD具有血統(Lineage)特性，記錄每個Partition的位置、父RDD的依賴關係以及產生方式，因此當RDD失效時，可以透過Lineage重新計算，實現容錯機制。Spark操作RDD的方式有兩種：轉換(Transformation)和行動(Action)。Transformation從現有RDD產生新的RDD，而Action則執行RDD運算，將結果返回客戶端或儲存至外部儲存裝置。
