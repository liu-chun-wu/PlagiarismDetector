決策樹以樹狀結構圖處理分類問題，目標是根據目標類標籤，以最佳方式分割數據集。分割時，選擇能使分類後類別更單一、數據集更純化的特徵作為判斷節點。從根節點開始，數據依特徵分割至兩側，分割原則以最大資訊獲利（Information Gain，簡稱IG）為基準。資訊量以熵（Entropy）衡量，代表系統的混亂程度。分割前後的資訊量變化如圖5所示。以屬性「月收入」分割數據集D 為DL 與DR 為例，資訊獲利計算公式如下：

Gain(D, 收入) = H(D)－〔P(DL)× H(DL) + P(DR) × H(DR)〕

資訊獲利（IG）等於原本的資訊量減去分割後的資訊量，子集的資訊獲利計算方式為：原本的資訊量減去左分割的資訊量再減去右分割的資訊量。資訊含量越小，代表越容易辨別。因此，分割後的資訊量越小，資訊獲利越大。
