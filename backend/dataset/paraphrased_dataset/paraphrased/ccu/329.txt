Apache Spark是一個開源叢集運算工具，允許用戶將資料載入記憶體進行儲存和運算。其核心是RDD（彈性分散式資料集），可與外部系統（例如HDFS）相容。Hadoop和Spark目的不同。Hadoop是一個分散式資料基礎框架，將大量資料分派到由普通電腦組成的叢集進行儲存和管理，實現高效能運算而無需昂貴伺服器。它也提供MapReduce功能進行分散式資料處理。Spark則專注於分散式資料的大數據運算，不負責資料儲存。由於缺乏文件管理系統，Spark需與其他分散式資料管理系統整合，例如HDFS或雲端資料平台。Hadoop搭配Spark被認為是最佳組合。Spark的資料處理方式使其比MapReduce更快。Spark將中間資料暫存於記憶體，批次運算完成後才寫回檔案系統，而MapReduce則分次讀取、處理、寫回資料。Spark的記憶體運算速度最多可比MapReduce快近100倍。
