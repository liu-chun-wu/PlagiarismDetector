This study uses the word_tokenize module from Python's Natural Language Toolkit for word tokenization of English articles.  To accurately calculate keyword weights in article summaries, keywords are pre-processed.  Without this pre-processing, word_tokenize would split keywords (as shown in Table 4), affecting subsequent weight calculations.  Therefore, before tokenization, article summaries are scanned for keywords, and spaces within those keywords are replaced with underscores.
